{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYzsHktkfVNvJ6wYNTE/kd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristofsl/BayesianHyperParameterOptimisation/blob/master/example_cola.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6keXOUa4AbS",
        "outputId": "b3f2a389-903f-484c-d32c-ba45f653f89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/404.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m399.4/404.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pandas scikit-learn optuna shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "J9G7PVYk4OJI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 2. ğŸ’¾ Data Loading and Initial Preparation\n",
        "\n",
        "# Load the Titanic dataset.\n",
        "# For simplicity and robust Colab execution, we'll use a standard UCI-hosted version\n",
        "# or load a pre-cleaned version often used in tutorials.\n",
        "\n",
        "# Since `pd.read_csv` from a direct URL is robust in Colab:\n",
        "data_url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "try:\n",
        "    df = pd.read_csv(data_url)\n",
        "    print(\"Titanic dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    # Fallback/alternative if the direct link fails\n",
        "    print(f\"Could not load data from URL: {e}. Trying a local version (if available).\")\n",
        "    # For a real Colab notebook, you might mount Google Drive or use files.upload() here.\n",
        "    # For this example, we proceed with the loaded data.\n",
        "\n",
        "# Display the first few rows to understand the data\n",
        "print(\"\\n--- Initial Data Head ---\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values and data types\n",
        "print(\"\\n--- Data Info and Missing Values ---\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHlvJgO34RUE",
        "outputId": "91c20999-0297-483a-84fe-c2aadaac3fcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic dataset loaded successfully.\n",
            "\n",
            "--- Initial Data Head ---\n",
            "   Survived  Pclass                                               Name  \\\n",
            "0         0       3                             Mr. Owen Harris Braund   \n",
            "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
            "2         1       3                              Miss. Laina Heikkinen   \n",
            "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
            "4         0       3                            Mr. William Henry Allen   \n",
            "\n",
            "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
            "0    male  22.0                        1                        0   7.2500  \n",
            "1  female  38.0                        1                        0  71.2833  \n",
            "2  female  26.0                        0                        0   7.9250  \n",
            "3  female  35.0                        1                        0  53.1000  \n",
            "4    male  35.0                        0                        0   8.0500  \n",
            "\n",
            "--- Data Info and Missing Values ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 887 entries, 0 to 886\n",
            "Data columns (total 8 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   Survived                 887 non-null    int64  \n",
            " 1   Pclass                   887 non-null    int64  \n",
            " 2   Name                     887 non-null    object \n",
            " 3   Sex                      887 non-null    object \n",
            " 4   Age                      887 non-null    float64\n",
            " 5   Siblings/Spouses Aboard  887 non-null    int64  \n",
            " 6   Parents/Children Aboard  887 non-null    int64  \n",
            " 7   Fare                     887 non-null    float64\n",
            "dtypes: float64(2), int64(4), object(2)\n",
            "memory usage: 55.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### 3. ğŸ§¹ Data Preprocessing\n",
        "\n",
        "# The goal is to prepare the data for the Random Forest model.\n",
        "\n",
        "# **3.1. Feature Selection**\n",
        "# - `Survived`: The target variable (what we want to predict).\n",
        "# - `Pclass`, `Sex`, `Age`, `Siblings/Spouses Aboard`, `Parents/Children Aboard`, `Fare`: Features to use.\n",
        "\n",
        "# **3.2. Handling Missing Values (Imputation)**\n",
        "\n",
        "# 'Age' has missing values (NaNs). We will fill them with the median age.\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "\n",
        "# 'Fare' might have a few missing values in some datasets (though not in the one we loaded).\n",
        "# It's good practice to check and impute if necessary.\n",
        "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
        "\n",
        "# Check again for any remaining NaNs (should be zero now)\n",
        "print(\"\\n--- Missing values after imputation ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# **3.3. Encoding Categorical Features**\n",
        "# 'Sex' is a categorical column ('male'/'female') and needs to be converted to numbers (0/1).\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfIhThWM41Wx",
        "outputId": "dc79790b-730d-4e18-febd-0c4623e0e282"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Missing values after imputation ---\n",
            "Survived                   0\n",
            "Pclass                     0\n",
            "Name                       0\n",
            "Sex                        0\n",
            "Age                        0\n",
            "Siblings/Spouses Aboard    0\n",
            "Parents/Children Aboard    0\n",
            "Fare                       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the data into training and testing sets and stratefy on y\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "JpTzRQSw5dXi"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}