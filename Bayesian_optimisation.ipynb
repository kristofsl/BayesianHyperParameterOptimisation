{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian function optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "from math import cos\n",
    "from math import pi\n",
    "from numpy import arange\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from numpy import asarray\n",
    "from numpy.random import normal\n",
    "from numpy.random import random\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from warnings import catch_warnings\n",
    "from warnings import simplefilter\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objective function\n",
    "\n",
    "This function generates the objective function that we don't know and we want to know the maximum of that function in the range 0 to 1. This is a simple function with 5 peeks and we only have one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unknown objective function where we want to know the maximum value between 0 and 1\n",
    "def objective(x, noise=0.1):\n",
    "    noise = normal(loc=0, scale=noise)\n",
    "    return (x**2 * cos(6 * pi * x)**12.0) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO29e5RbV5Xg/dtSqUqqt+2yHbvs+BXHIQ8SJyYEwiMJpJOGITE0DWG+DI9FwzTdMNNpJt3O1wxNA91kyMzQq2elH6EnTQ/z0SSEYAwETCChQ55tx3YedmLit6sc21V2vUtS6XG+PyRVySpd6Uq6urqS9m8tL5euju45R7p333322Q8xxqAoiqI0Pr5aD0BRFEVxBxX4iqIoTYIKfEVRlCZBBb6iKEqToAJfURSlSWip9QCs6OvrM6tXr671MBRFUeqK559/ftgYszjfe54V+KtXr2bnzp21HoaiKEpdISJHrd5Tk46iKEqToAJfURSlSVCBryiK0iSowFcURWkSVOAriqI0CSrwFUVRmgQV+IqiKE2CIwJfRG4Wkf0ickBEtuR5/3wReVxEdovIiyLyHif6VRRFUexTceCViPiBe4EbgQFgh4hsM8bsy2r2BeBBY8zficjFwCPA6kr7VhRFqXe27h7knu37OTEaZnlviDtv2sDmjf1V6csJDf9q4IAx5pAxZgb4LnBrThsDdKf/7gFOONCvoihKXbN19yB3PfwSg6NhDDA4Guauh19i6+7BqvTnhMDvB45nvR5IH8vmS8DtIjJASrv/XL4TicinRWSniOwcGhpyYGiKoije5Z7t+wnHEuccC8cS3LN9f1X6c0LgS55juXUTPwJ8yxizAngP8G0Rmde3MeY+Y8wmY8ymxYvz5v5RFEVpGE6Mhks6XilOCPwBYGXW6xXMN9l8EngQwBjzDBAE+hzoW1EUpW5Z3hsq6XilOCHwdwDrRWSNiLQCtwHbctocA94FICJvICXw1WajKEpTc+dNGwgF/OccCwX83HnThqr0V7GXjjEmLiKfBbYDfuB+Y8xeEfkysNMYsw34PPBNEbmDlLnn48aYXLOPoihKU5HxxvnC1peZjMZZ3hPkT26+qGpeOo7kwzfGPEJqMzb72Bez/t4HXOtEX4qiKI3E5o39vDQ4xgM7jvP0Xe+qal8aaasoilJjwrEEwRzTTjVQga8oilJjIjMJgoHqi2MV+IqiKDUmHEvM27ytBirwFUVRakw4liDUqgJfURSl4QnPqA1fURSlKYi4ZNJxxC1TURSlVriZbbJaRGJJFfiKoiiFyGSbzCQgy2SbBOpK6KsNX1EUpQhuZ5usFuqHryiKUgS3s01WC/XDVxRFKYLb2SarhfrhK4qiFMHtbJPVIJZIEk8a3bRVFEUpRGZjtp69dDJ7EG5s2qrAVxSlrtm8sb+uBHwukbTA101bRVGUBicykwRQG76iKEqj46ZJRwW+oihKDZkV+KrhK4qiNDbhmZTAb1M/fEVRlMYmohq+oiiKfe59/AB3//TVWg+jLNSGryiKUgI/33uSf3jiIAdOT9Z6KCWjGr6iKEoJjIZjGAN/+6sDtR5KyeimraIoSgmMTsfw+4Qf7jnB8bPTtR5OSWQ2bYNq0lEURSlMMmkYj8T4nSv78Yvwd/96sNZDKgk16SiKothkIhLHGNhwXjcf3LSCh3YOcHIsUuth2SYcS+D3CQG/umUqiqIUZDQ8A0BvKMBn3rmOhDHc98ShGo/KPuEZd8obggp8RVHqnNHpGAC97QFWLmznresW8dzhMzUelX3cqnYFKvAVRalzRsNzAh9gUUcrE5F4LYdUEtFYglCrO6JYBb6iKHXN6HTKpNMTagWgKxhgIhKr5ZBKwq1qV6ACX1GUOmcsR8PvDrUwHoljjKnlsGyjAl9RFMUmGRt+Tygt8IMBEknDdNq/3euEZ+rMhi8iN4vIfhE5ICJbLNp8SET2icheEfmOE/0qiqKMTsfobGuZdWvsCqYEf73Y8SOxhCt5dMCBEoci4gfuBW4EBoAdIrLNGLMvq8164C7gWmPMiIgsqbRfRVEUSLllZrR7SJl0AMYjMc7rCdZqWLYJxxIsa6kfDf9q4IAx5pAxZgb4LnBrTptPAfcaY0YAjDGnHehXURSFsenYrP0eUiYdgPFwfWzchl3U8J0Q+P3A8azXA+lj2VwIXCgiT4nIsyJyc74TicinRWSniOwcGhpyYGiKojQ6o+FzBX5XMKXh149JJ1lXNnzJcyx3e7wFWA9cB3wE+EcR6Z33IWPuM8ZsMsZsWrx4sQNDUxSl0RmdnqE37ZIJ0J0274zXiWtmZKa+vHQGgJVZr1cAJ/K0+aExJmaMOQzsJ/UAUBRFqYixcIyeujfp1E/g1Q5gvYisEZFW4DZgW06brcD1ACLSR8rEUz/JLhRF8STGGEanY/SG5pt0xuvApBNLJIknTf1o+MaYOPBZYDvwCvCgMWaviHxZRG5JN9sOnBGRfcDjwJ3GmPpJdqEoiieZmkkQT5pzbPjBgJ/WFl9dmHQyxU/csuFX7JYJYIx5BHgk59gXs/42wB+n/ymKojjCXFqFwDnHu4MBxsPe1/AjM+7VswWHBL6iKPXL1t2D3LN9PydGwyzvDXHnTRvYvDHX0c6bzEXZtp5zvDvYUhf5dGY1fJf88FXgK0oTs3X3IHc9/NKs4BkcDXPXwy8B1IXQz82jk6ErFKgLG34klgTc0/A1l46iNDH3bN8/K+wzhGMJ7tm+v0YjKg0rgV9vGn7dbNoqilK/nBgNl3Tca8wWP5ln0gnUhVvmbAFzFfiKolSb5b2hko57jdnyhrkafjpFsteZLWCuJh1FUarNnTdtmGdOCAX83HnThhqNqDTGpmO0tfjmacj1UgTFbZOObtoqShOT2ZitZy+dXO0eUjb8SCxJNJ6gzSUPmHLImHRU4CuK4gqbN/bXjYDPZTQ8M89+D3P5dCYicdo6PSzwZwOv3DG2qMBXFKVuGZ0+N49OJqZgML3p/INdg3zqHWtrNbyiZGz4QbXhK4qiFGYsPJdHJxNTMJjlYfTft+9n6+7BWg2vKBF1y1QURbFHtg0/X0xBNJH0dExBOJagxSez5RmrjQp8RVHqltHwDL3tKRt+PcYUhGeSrmn3oAJfUZQ6JRJLEIklZxOn1WNMQTiWcM1+DyrwFUWpU3LTKuSLKWjxiadjCiIx96pdgXrpKIpSp+SmVciOKchs3N5w0RJPu5yGXSxvCCrwFUWpUzK58LMDr7JjCi7/i5+zrCdYk7HZJRJPuOaDD2rSURSlThkNZ3Lhz4+0hVSpwwmP59MJzyRcS5wGKvAVRalTxqbzp0bO0B0MeL7MYSSWcC1xGqjAVxSlTpnLlDk/tQKkM2Z6vMxh2OVNWxX4iqLUJaPTMVp8QoeFhtxVBxq+CnxFURQbjIVTUbYikvf97mCgDmz4SfXDVxRFKcZoOGa5YQsZk463NXy3/fBV4CuKUpeMTRcW+F3BAJMzcZJJ4+KoSkMFvqIoShG27h7kucNn2HVslGvvfixvRszuYAvGwETUm2adWCJJPGnUD19RFMWKTBrkWCKluQ+Ohrnr4ZfmCf25IijeNOvMFT9RDV9RFCUv+dIgh2OJeWmQu4OpRAJedc2MzLhbwBxU4CtK0xNLJPkv33uBlwbGaj0UW9hNg9wdTGn4XnXNdLuAOajAV5Sm59evDfHQ8wNs33uy1kOxhd00yNl1bb2ICnxFUVxn6+4TAOeUBvQyd960gWDLuaIrFPDPS4PcNWvS8aiGP+NuPVtQga8oTc1UNM6j+04BMDAyXePR2GPzxn4+/1tzwr2/N8TXPnDZvDTIT742DMDnv/eCpSdPLamFhq/pkRWlifn5vpOEYwlWL2pncKQ+NHyAa9YuAuCbH93EjRcvnff+1t2DfOXH+2ZfZzx5AM/kx4/GkoB66SiK4hJbd5+gvzfEv3vjck6OR4glkrUeki0yrpYZs00u92zfTyR+7lzyefLUkrq14YvIzSKyX0QOiMiWAu0+KCJGRDY50a+iKOUzPBnlyQPD3HrFclYsCJE0cHIsUuth2WI8vRHb2ZZf4NdDQfOMDb+uBL6I+IF7gd8GLgY+IiIX52nXBfwn4LlK+1QUpXJ+/MIJEknD5o39rFjQDsBAnZh1JtPRsxnXy1zqoaD5bOBVa31F2l4NHDDGHDLGzADfBW7N0+4rwNeB+lAhFKXB2brnBG9Y1s2FS7voX5AShPXiqVPMpJOvoHk+T55aUpcaPtAPHM96PZA+NouIbARWGmN+XOhEIvJpEdkpIjuHhoYcGJqiKPkYmZphz/FR/t0blwHM1n6tl43bjG99p4XA37yxn6994DLa0u6bVp48tSSzSulodc93xome8iWjnk1PJyI+4BvAx4udyBhzH3AfwKZNm7yb4k5R6pxTE6mF9upFHUDKU2RxVxuDo/XhmjkRiREM+Aj4rXXWzRv7eXz/aV44Psqv7rzexdHZYzIap7OtBZ8vfz7/auCEhj8ArMx6vQI4kfW6C7gU+JWIHAGuAbbpxq2i1I7hiVR5wL7OufKAKxaE6sakMxmN02Vhv8+mK9gyu8HrNSYjcTra3DPngDMCfwewXkTWiEgrcBuwLfOmMWbMGNNnjFltjFkNPAvcYozZ6UDfiqKUwfBkFIC+rrbZY/29obrZtB2PxC3t99l0BwOMh2MY4z2DweRM3NLLqFpU3JsxJi4inwW2A37gfmPMXhH5MrDTGLOt8BkURXGboYmUwF+cLfAXhPj53lMkkwafT9i6e5B7tu/nxGiY5b0h7rxpg2ds4BMRexp+dyhAPGkIxxK0u2grt8NkJE6njTk4iSPfgDHmEeCRnGNftGh7nRN9Kkqt8LIgtMvwZJTWFh9dWRrmit4QM4kkQ5NRnjl4hrsefmnWddBrkaoTkdg5Y7ci47Y5EYl7T+BH47bm4CQaaasoJZApvjE4GsZgXXzD6wxNRlnc2XZOAfBsX3y7OedrxaRNk46XE6hNRtw36ajAV5QS8LogtMvQRPQc+z0w64s/MDLt+UjVCbs2/JB3c+JPRuOWbqXVwltrHEXxOF4XhHYZnpyhvzd4zrH+3rngq+W9+T12vBKpOhGJ2bPhO1z1yklz3kQkphq+oniZegjZt8PwZJS+znM1/I62FnrbAwyOhD0dqZpIGqZmEraEpZMavpPmPGPMrB++m6jAV5QS8LIgtEsiaTgzGT3HQydDxhc/E6na3xtC8FakaiZC1a5bJuCIL76T5rxwLEHSWEcKVws16ShKCWQEXj176YxMz5A0zNPwISXYDw5NAam5enFemTw6VonTsnFy09ZJc17moVV3fviK0mx4VRDaJeODn1/gt/PEb4YxxpzjweMlMnl07Gj4wYCf1hafIyYdJ/c1JkuYg5OoSUdRmoxMlG0+k07/ghDhWIKRae95tWQoljgtl+5gwJFC5k6a81TDVxTFFWbTKmTl0cmwIpMmeSTMwo7573uByWgmNbK9KNXuUIsjJh0nzXmTRQq4VAsV+IrSZORLq5Ah45o5MDLNZSt6XB2XXUox6aTaBRxLoOaUOW8iWtoqxSlU4CtKjahViobhyRnaWnx5tcsVdVAIJSO87aYl6A46o+E7yawNv60Oc+koilIaGZ/uWuSqGZ5I+eDn25TtCQVo9fsYSpt9vMhctSu7Jp2A5x5gs8VP6jA9sqIoJVLLFA1DFj74ACJCX2crZyZnqj6OcpmMxGnxCcGAPfHl1Katk0zWyKSjAl9RakAtUzQMTcyPss1mUWfb7MauF8nk0bHrNurUpq2TTEbjtPp9tLWohq8oDU8tUzQMT86wuMvaA2eRxzX8iUisJM24OxggGk8SyVlR1ZJULnz3Leoq8BWlBtQqRUMiaTg7lUqNbEVfPWj4JWx2ZhKoecmsU4s8OqCbtoriGKV43dQqRcPZqXRaBQsbPsxp+F6Ntp2I2kuNnCGTQG0iErPcu3CbiRrkwgcV+K7SCJWSlPyU43VTixQNc0FXBTT8jjZmEkkmonFb+WrcInP/DI6GCbb42Lp70Nb352QCNaeYjJZmlnIKNem4RKNUSlLyUy+FUQoFXWXoS9v3hye8Y9bJvn8AIvGk7fvHi1WvalHeEFTgu0a9CASlNLbuHuTaux+z9PP2WmEUOxr+oo7Ue2emvLNxW8n948WqV5OROB1q0mlcGqVSkjJHrhknH14rjFIoj06GzMPASxp+JffPrEnHoapXTlCL8oagGr5rNEqlJGWOfFpnNl4sjDI0ESUYyJ9WIUPmYTDsIQ2/kvunO5Tx0vGOhp/yNFKB37A0QqUk5VwKaZdeqhCVzfDkjGVahQwL0lkyz3jINbOS+ycU8NPiE8+YdGKJJNF4Ur10GplGqJSkzPFvh8/S4hdiCTPvvf7eEE9tuaEGoypOsShbgIDfx4L2gKd88TP3yV898gqnJ6IsaA/w5++7xNb9IyJ0BVs8Y9KZqlFaBVCB7yr1XilJmeMHuweJJQwBnxBLzgl9r6/ahiejrFzYXrTdos42z0Xbbt7Yz6pF7bz/b5/mf37oCq6/aIntz3aHAp7R8CdqlAsfVOArSlk8eWAIgGW9IRJJUzertuHJKBvPX1A0JqSvs9VTGn6GUnPhZ/BSArVSirA7jQr8BkIDu9zh2Jlpjp8Nc8GSTg6cnuTRO97B+qVdtR5WUeKJJGemZjg7FS0aJLaos41XTozXbKxWlFreMIOXEqjNpUbWTVulTDSwyz2ePDAMwF+9/zJafMJDuwZqPCJ7fOe5YxgD2/eeKurT3tfhVQ2/tFz4GbravGPSqVV5Q1CB3zBoYJd7PHlgiGU9Qd60egHXbVjCD3YNEk8kaz2sgmzdPchfPvJKwTbZXkd9nW2MR+JE497JMAnlm0NSGr43TDqZ8ob/8dvPs2bLT7j27sdcU8xU4DcIGtjlDomk4emDZ3jbBX2ICB+8agWnJ6L8Oq31e5V7tu8nGi/8UMr2aV+U9uQ56yFffJjLh9PZWroN3ysa/pOvpfZ/Tk9EXV+Nq8BvEDSwyx32nhhjdDrG29b3AXDDRUtY0B7goee9bdYp9uDP9S5a1JnxxfeWwJ+IxOhsa8HnKy2LZ3cowPRMwhMrsZ+9fHLeMbdW444IfBG5WUT2i8gBEdmS5/0/FpF9IvKiiPxSRFY50a8yhwZ2uUPGfv/WdSmB39ri49Yr+nl07ynPaJD5KPTgzxcklvHV91pt20y1q1LxUk58q6ydbqzGK941EBE/cC9wIzAA7BCRbcaYfVnNdgObjDHTIvIZ4OvAhyvtu5ko5oGjgV3u8NSBYS46r+ucbJPvvHAx33r6CPtPTvCm1QtrOLoU+a6VO2/awOcffIGEOTdmIFfQZ6cgBnh07ymu32Df373aTJYp8LuCcwnUMpHEtaKjzc9UdP7eiBurcSe2ia8GDhhjDgGIyHeBW4FZgW+MeTyr/bPA7Q702zTYzbWugV3VJRJLsOPICB+95twF6rrFnQAcGpqsucC3ula+9oHL2HBeF6+dniCeMHkVgnzJ4B7ceZyr1yz0zHU1EY2V7KEDWRkzPbBxe+nyHv7t8FmyY7TdWo07YdLpB45nvR5IH7Pik8BP870hIp8WkZ0isnNoaMiBoTUG6oHjDXYcOctMPDlrv8/QvyBEa4uPg0NTNRrZHIWuFRF4+/rFHL77vTy15YZ5QjzfZ+NJ46nrbHQ6NmueKYU5k07tzW6LOltZ0t1Gf28Iwd28S05o+Pl2T+YnGAFE5HZgE/DOfO8bY+4D7gPYtGlT3nM0I+qB4w1eOD4KwKYcLd7vE9Ys6uDg6claDOscCl0r0XiCN67oLeuzXuH0RJTL+ntK/pyXcuJPROIs6wmx9Q+vdb1vJzT8AWBl1usVwIncRiLybuDPgFuMMd7aCSqTTPGLavvSqgeONzg0NMWynmDegJl1Szo4NFx7Dd/qmljWE2R4coal3daJ07x+ncUTSYYnoyzpDpb82bmqV7U36UyWWJPXSZwQ+DuA9SKyRkRagduAbdkNRGQj8A+khP1pB/qsOW5GtqoHjjc4ODQ5a6/PZd3iTo6dnWamiK97tbG6Vj71jrUALC0gLPN9VtLHvcDw5AzGUPChZYWXNPzJGhUwBwcEvjEmDnwW2A68AjxojNkrIl8WkVvSze4BOoHvicgeEdlmcbq6wU27+uaN/XztA5fVxOanpDDGcGhoirWLO/K+v3ZxB4mk4djZ2mj5mdXmHQ/soa0lld44+1q5YmXKlFNIWOZeZ+2tfrqCLZ65zk6NRwBY2lW6ht/Z2oKINwqZT0VrJ/Ad6dUY8wjwSM6xL2b9/W4n+nECpxKMuW3vVA+c2jI0EWUiGmdtX36Bn9H8D5ye4oIllSdSK+U6zfWuGQ3HCAX8fOPDV8x+JhPss6SIsMy+zr7+s1e574lDJJOm5ECnajAr8Msw6fh8QlebNxKoTdSovCE0WaStk2YYr9s7FWfJeOCsW5LfpLM2LfAPDlW+cVvqdWpntXl6onRhuaizjXjSeMIMAtkCv3STDngjJ74xJmXDr1eTTj3hpBlG7erNRUaQr7Ww4Xe2tbC0u41DDrhmlnqd2lltnhqP4PcJi0oIOpqtbeuRaNtT49HUHIpU7LKiKxio+abt9EwCY2pT7QqaTOA7aYZRu3pzcWhoilDAz7ICGvK6xZ2OaPilXqeFVpsZ2/69jx8EA9temOdAZ0kmvcKwR/LpnBqPsLizDX+Z5qXuYEvNNfxa5sKHJiuAsrw3NBsynnu8HNSu3thk29FbW3z0dbYVtGWvW9zJ1j2DGGMKFgkvRqnX6Z03bZgXIRsK+Ln+osXnHE8YkzdC24pFXtPwJ6Jlm3MAekIBjp6ZdnBEpVPL8obQZBq+mmEUu+Ta0aPxJCfHIgX3e9Yu7mAiEq9YIy71OrVabT7+6lBFJsyMhu+VjJmnxyNl+eBnWN4b4sRoGGNqF9NZy/KG0GQaviYYU+ySz46eMKk0A1bXy7qsjdvs5GqlUs51mm+1eccDe/K2tWvCXNDeSsAvvD4WsTny6nJqPMKm1QvK/vyKBSEmonHGw3F62kvPx+MEc9WuatN/Uwl8UDOMF/By7d3cbJG5FBKWGQ+eQ0NTXLN2UUXjcOI6rdSE6fcJKxe0c/RM7SOIo/EEI9OxsnzwM6xY0A7A/3nmCN/dcbwm199kNLWHoCYdpSmoVoSyE2kussdmhQHL8y/rDhIM+BzZuHUCJ0yYq/s6OOyBlBGnx1P7COX44GdYsSD1oPtfjx2oWe3nyXRa5HpOraAotqlGhLJTD5F8Y8uH1fl9PmFtnzOeOlaU8mDL2PYz7pWLOlpL9iRbtaido2ema2r3hrk4giUVbNquTGv4MzlVr9zMPDsZUQ1faSKqEaHs1EOklDFYnX/dkk5HfPHzUc6DbfPGfr50yyUAfOdT15RsuljT10E4luD0RG09dU6OVa7hF7Lbu5URdHhyBr9P1A9faQ6qEaHs1EOk1DHkO//avg6Oj0wTsbFSKJVyH2ynZs0hpWvHqxelUknYMetUM3tsJWkVsglYuNW6FSF/+MwUKxaECPhrI3pV4CuuYYzhjnevd9w11qmHSD6bd7DF+hbJd/4LlnRijDMpFnIp98F2ejxCa4uPnlDpniFr0rmDim3cVjt77KmJCK3+VFK4SrhoWfe8Ah5uumYfGZ6a/U5rQcMKfLdy1Sv2+YP/bxf/5aEXSRozGy3pRISyU/EV+fzZ73rPG4D5mqHV+S9Z3g3A3sHxkvq2Q7kPtpPjEZZ2t5UVDLasJ0jALxweLhywVO3ssafHoywpcw7ZvGn1QgItPpb3BF2PkDfGcGR4anbVVAsa0i3Tbg1YL7sHNhrHzkzzs70nueGiJaxa1M7uY6O8PDjGD/7wrUUzOBbDyfiKXHfIpw8OA/B7b1/LthdOFD3/6kUddLa18NLgGB9608p571eCVURtsQfbqfFI2e6MLX4fKxe2c6SISafa2WNPjUcqNucArFwYYiae5Eefe1vZOXnKZWgiytRMwjLFths0pMAvpG1kblK7DwXFGb70o70YA4+9epr+3hAffcsq9hwf5aHnB/iD6y6o+PzViq/IlC28/S2r+NPfvqhoe59PuGR5Ny8Njjk+lnIfbKfHo7xhWXfZ/a5Z1MGRIiYdp9OW5HJqPMKG8ypPO53xxR8YCbsu8DP7IKrhO4wdbcPOQ0EpnXyrplgiyWOvzhU6GxwN89e/eI11izt4YMdxfv8d6zyRbz0fe46PsbCjleU99rXLS/t7+L/PHiWeSNLi8OZcOQ+2U+MRrtuwpOw+Vy3q4OmDZwrmCCp39WGX0+NR3r5+ccXnyfjiHx+Z5vKV1vV9q0FG4NfSht+QAt+OtlHNJWizmoqsVk35ZEQ4luDs1Awj0zGePXSGt17Q5/Jo7bH72AhXnt9bku34sv4eovEkB4Ymuei88jVrJ5iMxpmaSVSUdGxNXzvhWIJT41HOs3jwVTNtyVQ0zkQ07ohJJyPwB0bcL8x++MwUrX5fTWtmNKTAt6Nt2F2Cliq8m9lUZLVqsmJkOkZPKMB3/u2YJwX+yNQMh4an+OCmFSV97tL+lJB/aWCs5gI/Y5JaubC97HOsTmukR85MWQp8qJ5ZLRMDUMlDK/s+FoEnfjPE779znVNDtMXTB86QNIYL/t9HaqYINqSXjp1c9XY8O8pxNXOz1q3XKHV11N8b4v0b+/n53lOcnTo3I6ObXlZWfe0+PgLAleeXlrBrTV8n7a1+9p5w3lOnVJ4/mprDxvPLN19kbM7FNm7tUM7venKsMh/83PvYGHj20BlXPfe27h7k5cEx4klTk5QOGRpSw4fi2oadJWg5dn63a916CatVE6T82SPxuZD2zMP19dEwM4kkV37lUfrTvwHg2iqp0IrstdMT+H3CG1f0lHROfxU3bkvl+WMj9PeGWNZTvhlheW+IVr+PwxUmUSt39TtXnrE8DT/ffZw0uLpf9/WfvUpucopa7Bk2rMC3Q7GHQjnCu9reCl4mnykNYNOqBdx+zap5D1eAv3nswGy7jAAIBnyubagXeqifv7Cdi5d1095a+m1yyfIeHthxnETSlF2hyQl2Hx3hylXlpxSGdNbMhSGOFvHFL0a5jhKZKNtyc+Fb3a+FkuQ5zXNpoqUAABXUSURBVAmLFNNuK4JNLfCLUY7wrra3gpfJXTX1tgcYmY5xx40Xcu0FffNu6mvvfiyvALCy+1fj5igkDEamZ/jdq0qz32e4rL+Hbz19hENDk6xfWrk7YTmcGA1zYizCpyoU+JDyLCnmmmlnPKUcz3BqPEp7q7/swt9W9/F5DmwC22VRRytnpuYXknFbEWxIG75TlBPBmbt/0BsKEAz4uOOBPbZslvUeIbx5Yz9PbbmBw3e/lytW9rKsJ2iZG77auW4KkfmerXJALu5sY3omUbZ2fFnaDFRLs86uYyn7/VUOCPxVaV/8ZLL8rJnlRgpngq7KjbLNdx8DfNjhwLhCXJ/HLbYWiqAK/ALk2/z9nav6uWf7/oICOSP0vvHhK4jGk4xMx2xt1FQ7H4mbnJ6I8MRrw7x/Y7+lScPqRu8NBapairJY3vtQwM87L0z5fJe6YZthbV8HwYCvpgL/+aMjBAO+ioKuMqzu6yASS1aUNbPcFBgnxyIsqbCCWPZ9nDmXmxGv3aEArTVK6ZCNmnSKkG3nL3XTqZjHTq5Nu5GCwbbtOUEiafjAldbjzmf+avX7ZtP5ViuWoVDe+8zG8b/+ZojFXW2zftul0uL3cfGy7qrk1LHLrqMjXL6i15HMjGuysmYWcs0sRDm++mPhGC8OjPHRt6wqq8/svjP9TM/EufiL213xxc+uoBbwCX9y80U1vZdV4JdAqQK5kH0434PDSdt1tt9xTyiACIxOx1zz/3141yDnL2znY/fvsLy5swXA4GgYn8CFSztnj1fDI6dQ+UIBntpyAwDf+MVvSg64yuWy/h4een6AZNK4HkkciSXYe2KcT71jrSPnW7Uo5cd/5MwUb1lXfvnGUn31f773JDOJJO+7fHnZfebS3tpCX2crAyOVbUIXI1dBjCVNzWNyVOCXQKmbTlabRX6RvA8OvwiJPJWFCtk48wWGwblujaPh2Gx7NwLBXnl9nH2vjxPwCbG0zdeq32wB8FePvML9Tx7mzGTU8TwnuTdfPjLf8/BklKNnpvl/3nx+RX1e2t/DPz9zlJcGx1wP439xIOXzfVWZJqlclveG6A628OSBYT5ydWXfSyn845OH8fuEW+99anb15cR127+gneNnq6vhe3HF3hQ2fDsboXbalLrpZGWzzCfUARLGWNo4p6JxHt9/+pwHiJXN/y9+tLegYCsWCGb1XdjdUH541wDArLC32++CUCAlpL76C8c3rIuVL8y2Je8+NgqUb7/P8FuXnEdXsIW//dWB4o0dJhNwValLZga/T/jIm8/npy+9zvGz1dWMM3z7mSPsPzlBIkdpcOK6uGBxJy8OjDIRiRVvXCZejMlpeIFvZyM0X5s7HtjD6hzBVmjTKZ8wtIr47S+gsbe1pIo8CLC8J8htb1rJz14+yVVffZRP/NMO3v7fHuP3/nkHj+8/zdd/9mpeDWJkuvhFPDgazitUrb6vL2x9ydaG8qGhSb797FHLfq0u9q27B/P65Dsl9AvdZNkbaMYY/vHXh+gJBbi0v7SAq1x6QgE+ce0atu89xSuvu2vLf/7oCGv7OljY0erYOT/x1jX4RLj/qcMlfa5cz7P//vPfzDvmVNT6x9+6mvFInG89daTic1lRjepuldLwJh07y6p8bTK6aT5TRDETSu5n8i3frMwLo+EYoYCfr26+lH/9zRD/9PQRlna3cdubzue6DYvZceQsD+w4zi9eOT3vs6ViNbd839e/PHd83sok93uMJ5L88YMv0NbipzfUysnx+cEmVhd7tZe/Vua1/t7QrN0e4Pu7Bnnu8Fm+9oHLCOZx5SuVT167hn968jB/88vX+Lvbr6r4fHYwxrD72EhFGTLzcV5PkFuuWM4DO47zR++6sGCN2AyV5JYaC+dXXJzQkC9b0cONFy/lm78+xEffurqsamDF8mx5MSan4TV8O8uqYhdQtlaR7Wf+1JYb2Lyxv+T8Odmav1V/X/zhXh7ff5ovvPcNPLPlXXzplku4bsMS7rzpIp7e8i6+uvnSeaXaMuRza7QzN7D+LqzMUNnt//5fD7Ln+Chf3XwpW377opJc8Kq9/LXjEjgyNcNfPfIKV61awIc3OeOj3dMe4OPXruanL5/k1ZPuaPnfe36AM1MzvGm1M+YcmNPSH941yPRMgj/b+pKtz5WbW+r1scLR7E7wR+9ez3gkzv1PlrZiAXuWg80b+/mD6+YStC3vCdbEFTMbRwS+iNwsIvtF5ICIbMnzfpuIPJB+/zkRWe1Ev3aws6yycwEVEjzlCKvMg8NKaCeM4eHPXMvvvX3tPA+P1hYft1+zij+5aUPe+pxfuuWSecFfhWqBZo/T6rvwW3irZNq/PDjGX//iNd53+XLed/lyWwns8p0nlyUVZEjMxs54vvbTVxgPx/jL91/qqFfNJ9+2hs62Fv7XL5215eczlfzohRNs+f6LvC1PZHMl/eTGLfz4xdd5aOfxop8tdG8UMvX85MXXgZSJMxsnNeRLlvfw25eex/1PHmbMhhk0GzsPspGpGb674zjLe4I8/4V38/Rd76q5e3XFJh0R8QP3AjcCA8AOEdlmjNmX1eyTwIgx5gIRuQ34b8CHK+3bDnaWVVY5YLIp9FCoJH/OeT1BXs+TZ2NZT3A2WtOKz1x/AQs7W/nyj/YxNZMa+9VrFrKmr4MLl3blTWVQbJxW39fvXNXP958fnHf8P1yzij//4ct8f9cgCzta+cqtl8y+X4oLntVvkEgavvnEId53+fKy/b8LjccYw8GhSX760kke3DnAf3znWsdTGve2t/Kxt67ib391kP/0L7v5928+nzevWViRy2c+U8mfPPQi8WSSTasWct9Hr3LEJAXWG97/9Yd7Wd3XwZXnL7B8QFrdGz2hQF5Tz1g4xlg4xreePsIly7v51NvXVrW2xH9+93p++vJJvr79VT53w3rb11gxJS+RNPznB/YwNBHle7//Ftera1khxmKpbvsEIm8BvmSMuSn9+i4AY8zXstpsT7d5RkRagJPAYlOg802bNpmdO3dWNLanDgzzT08d5vWxCAdOTxKNJ2lr8bG6r+OcyD0RYWgiwqGhKaJZGR0z+ATesKzbMuPgybEw+14fJ9spxSdw8bJuzusJAQZjUvsCiaQhks4XMx6OcfTM9Lzw/lDAX1Abzmc7vGrVAv7ml6/x/V0DJA2IwKqF7SzqbKPV76Mt4GNoIsorBcc5N58DpyeJxJMEW3ysW9LJed2pB9PBodT3GPALPaEAw5MztPp9vPeNy/jD69dxwZLy88bkzut9ly/jmYNneGFgDBHYsLSLjrYWQgE/rS0+BLKKqxQTnnOTThqIJZLEEkkGR8Oz7nlXr1nItz7xprKSpRVjeibO13+2n+/vGmAiEmflwhBLu4IE03M5V14Wn8uTrw2fk300Q8Av7PqvN9IVLN0mbcWaLT+xTEEBqZw0a/o6aPELAb8PX9aDzOre8PuEWML6rO+4cDF/evMGLlle2ca5HT7/4At8P+1ZtrwnyLolnfh9gl8kfX3N/z2efG0o7/ff1uLj/IXtnByPMBGJ87UPXOaqGyuAiDxvjNmU9z0HBP4HgZuNMb+Xfv0fgDcbYz6b1ebldJuB9OuD6TbDOef6NPBpgPPPP/+qo0etvT3s8Oi+U3zj0d/g84GQ+vFEBJ+kfsJMbmxD6o+kAYNhdDrGqfEIsYQh4BeWdgXpLrKpMxaOcXpi7jNLuoLnbASJgC99AQUDfjpa/XS0tbBhaRfTMwm27hnk5FikqBaTz588+wHx+liYF46P8erJcfafnGA8EmMmniQaT5I0htGpGKcKjNOK1HeX+h7bWnx0BlvobGvhsv4efnfTSke9QXI5NDTJD/ecYO+JccKxOOGZBDOJJJlL1+4lnJFDIhDw+wj4Ux5Rb1+/mOsvWlLQe8opwjMJfvziCX6+7xRT0TiRWOIcJcPuXPYV8Po5cvd7Kx3mOVitDJf1BPnTmy/iZy+fZHgySixpiGf9Lhny3RuFMlUu7Wrjrve8wTXzRyJpeHFglF3HRtl1bISBkTBnJ6OcHLe+T8bCMU6k7fcZhFSA2kXndbOku43LV/TygSv7K1rJlUO1Bf7vAjflCPyrjTGfy2qzN90mW+BfbYw5Y3VeJzT8RsTq5sv1NlG8QzVKXrp5HRRTMqw+U2jOVuO3e/5qYne+Xi1lWkjgO7F2HQCyXRpWACcs2gykTTo9wFkH+m5o8l1QXgzmcJNap4wolWqVvHTT5a/UHDh25lxs36yWEal2XYSrVdKxmjgh8HcA60VkDTAI3Ab8+5w224CPAc8AHwQeK2S/V6xvmkyO+VxqGczhlqaT+524nTKiHKoVX1DNouFW/dk9t5055+ZRyketlJhGVqoqFvjGmLiIfBbYDviB+40xe0Xky8BOY8w24H8D3xaRA6Q0+9sq7bfRsbpp2lp8hAJ+zwRzuFm0vVh6hFrnKcmHXeFR6KFp9Z5XNUy7c86M38q8Y0iZftxeuTVy1TpH/PCNMY8YYy40xqwzxvxl+tgX08IeY0zEGPO7xpgLjDFXG2MOOdFvI2N104yFYyX5t1cbN4u229GwvKaF2YkDKRTEU481EpzIOZWhFvMtN29/PdDwqRXqlUJahpc0OzeXv4WKpGe38RJ2bO3FHppey7hYjFL3F4qZd9yeb6XmMq9u5oIKfM/ixTwc+XBz+Vtso8+L348d4VHOQ9NrK5lsyhGYGSXGyue/WvN12lzmpomzHFTgexS3N+XKpZbeIvXgpQPFNzyLPTTr0Z5crsB0U4GohnD2Yg78bFTgexg3TTelLkNz3SODAZ8rgtdL5iynKPbQrIeVnlO4qUBUQzh73cNHBb5SsqaTzz0yFPDzjQ9f0XDC2A3srOa8vtJzCjdXttUQzl738Kk40rZaaKSte5QatanRvsXx8sadkqIa13E5UclOUyjStuHz4SvFKVXT8fqytdbUoytlM1IN98tS04K7jZp0lJKXoV5fttYar2/cKSmqZT7y8j6TCnyl5I2yenEZrRW6AqofvCycq4EKfKVkTadeXEZrha6Amo962bPRTVtFcRgvbNwp7uG137va6ZGVBqZeNBcrajH+ZloB1fv14QT1tGejAl+xxOth4sWo5fibwTZc79eHU9TTno26ZSqWuJkJ00m27h7k2rsf448e2FOX468X6vX6cJpSs4PWEhX4iiVe0VwyAnzNlp9w7d2PFfRnz/aBt8KLmlc94pXro9bUUzplFfiKJV7QXEoNYipWJAW8qXnVI164PryA14OtslEbvmKJF/ztS90QK6ZdelXzqke8cH14hXrZs1GB38QU87DwgrdJqWaDQkVS+pvUi6RaeOH6UEpDBX6TYtfDotaaS6lBTFZap1eX2PVOra8PpTTUht+k1IuHRakbYl63p5ayAa0oTqMafpNSLx4WlZTL8xrqt67UGhX4TUo95XvxqgAvlXqKyFQaEzXpNCn15DvcKNTLqkppXFTDb1LUw8J96mlVVYhGzZ9jNa9Gmq9my1QUl/BaVsVyaIQ55MNqXr9zVT/ff36wruarJQ4VxQN43YPIDvXi3VUqVvP6l+eON9R81aSjKC5S7xvQjboPYTX+hIUFpF7nqxq+oii2adT8OVbj94uU1N7rqMBXFMU2jerdZTWvj7x5ZUPNV006iqLYplG9uwrNa9OqhQ0zX/XSURRFaSCq5qUjIgtF5FEReS39/4I8ba4QkWdEZK+IvCgiH66kT0VRFKU8KrXhbwF+aYxZD/wy/TqXaeCjxphLgJuBvxaR3gr7VRRFUUqkUoF/K/DP6b//Gdic28AY8xtjzGvpv08Ap4HFFfarKIqilEilAn+pMeZ1gPT/Swo1FpGrgVbgoMX7nxaRnSKyc2hoqMKhKYqiKNkU9dIRkV8A5+V5689K6UhElgHfBj5mjEnma2OMuQ+4D1KbtqWcX1EURSlMUYFvjHm31XsickpElhljXk8L9NMW7bqBnwBfMMY8W/ZoFUVRlLKp1KSzDfhY+u+PAT/MbSAircAPgP9jjPlehf0piqIoZVKpwL8buFFEXgNuTL9GRDaJyD+m23wIeAfwcRHZk/53RYX9KoqiKCXi2cArERkCjjpwqj5g2IHz1BPNNudmmy8035x1vvZZZYzJ6wnpWYHvFCKy0yrqrFFptjk323yh+eas83UGTZ6mKIrSJKjAVxRFaRKaQeDfV+sB1IBmm3OzzReab846XwdoeBu+oiiKkqIZNHxFURQFFfiKoihNQ8MIfBG5WUT2i8gBEZmXpllE2kTkgfT7z4nIavdH6Rw25vvHIrIvXYPglyKyqhbjdJJic85q90ERMSJS1258duYrIh9K/857ReQ7bo/RaWxc1+eLyOMisjt9bb+nFuN0ChG5X0ROi8jLFu+LiPxN+vt4UUSurKhDY0zd/wP8pDJwriWVjfMF4OKcNn8A/H3679uAB2o97irP93qgPf33Z+p5vnbnnG7XBTwBPAtsqvW4q/wbrwd2AwvSr5fUetwuzPk+4DPpvy8GjtR63BXO+R3AlcDLFu+/B/gpIMA1wHOV9NcoGv7VwAFjzCFjzAzwXVK5+rPJzt3/EPAuEYuS9N6n6HyNMY8bY6bTL58FVrg8Rqex8xsDfAX4OhBxc3BVwM58PwXca4wZATDG5E1eWEfYmbMButN/9wAnXByf4xhjngDOFmhyK6k8ZMakEk/2phNVlkWjCPx+4HjW64H0sbxtjDFxYAxY5MronMfOfLP5JCktoZ4pOmcR2QisNMb82M2BVQk7v/GFwIUi8pSIPCsiN7s2uupgZ85fAm4XkQHgEeBz7gytZpR6rxekaHrkOiGfpp7rb2qnTb1gey4icjuwCXhnVUdUfQrOWUR8wDeAj7s1oCpj5zduIWXWuY7UCu7XInKpMWa0ymOrFnbm/BHgW8aY/yEibwG+nZ5z3hobDYCjcqtRNPwBYGXW6xXMX+rNthGRFlLLwUJLKS9jZ76IyLtJFaq5xRgTdWls1aLYnLuAS4FficgRUvbObXW8cWv3mv6hMSZmjDkM7Cf1AKhX7Mz5k8CDAMaYZ4AgqURjjYqte90ujSLwdwDrRWRNOv/+baRy9WeTnbv/g8BjJr0rUocUnW/avPEPpIR9vdt2ocicjTFjxpg+Y8xqY8xqUvsWtxhjdtZmuBVj55reSmpzHhHpI2XiOeTqKJ3FzpyPAe8CEJE3kBL4jVwPdRvw0bS3zjXAmEmXlS2HhjDpGGPiIvJZYDupnf77jTF7ReTLwE5jzDbgf5Na/h0gpdnfVrsRV4bN+d4DdALfS+9NHzPG3FKzQVeIzTk3DDbnux34LRHZBySAO40xZ2o36sqwOefPA98UkTtImTY+XseKGyLyL6RMcn3pfYk/BwIAxpi/J7VP8R7gADANfKKi/ur4u1IURVFKoFFMOoqiKEoRVOAriqI0CSrwFUVRmgQV+IqiKE2CCnxFUZQmQQW+oihKk6ACX1EUpUn4/wEH+qNoetOKxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can plot this function with and without noise ...\n",
    "X = arange(0, 1, 0.01)\n",
    " \n",
    "# sample the domain without noise\n",
    "y = [objective(x, 0) for x in X]\n",
    " \n",
    "# sample the domain with noise\n",
    "ynoise = [objective(x) for x in X]\n",
    "\n",
    "# plot the points with noise\n",
    "pyplot.scatter(X, ynoise)\n",
    "\n",
    "# plot the points without noise\n",
    "pyplot.plot(X, y)\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The surrogate function\n",
    "\n",
    "We need to specify the surrogate function. For this we will use the gaussian process algorithm and we threat the problem as a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surrogate or approximation for the objective function\n",
    "def surrogate(model, X):\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        prediction = model.predict(X, return_std=True)\n",
    "        return prediction \n",
    "\n",
    "# plot real observations vs surrogate function\n",
    "def plot(X, y, model):\n",
    "    # scatter plot of inputs and real objective function\n",
    "    pyplot.scatter(X, y)\n",
    "    \n",
    "    # line plot of surrogate function across domain\n",
    "    Xsamples = asarray(arange(0, 1, 0.001))\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    \n",
    "    # The surrogate returns a prediction and a standard deviation for each point\n",
    "    ysamples, std = surrogate(model, Xsamples)\n",
    "    \n",
    "    # We only plot the prediction values\n",
    "    pyplot.plot(Xsamples, ysamples)\n",
    "    \n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 random values for X :\n",
      "[0.66612502 0.80710338 0.05264232 0.69110573 0.6545955  0.37860237\n",
      " 0.66280917 0.09519841 0.9073387  0.8109561 ]\n"
     ]
    }
   ],
   "source": [
    "# we take 10 random values\n",
    "X = random(10)\n",
    "print('10 random values for X :')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 generated values for Y with the objective function :\n",
      "[ 0.63991744  0.11909587 -0.02147565  0.2107197   0.40697611  0.18079028\n",
      "  0.55677467 -0.02579045 -0.01050622  0.08870031]\n"
     ]
    }
   ],
   "source": [
    "# we generate 10 y values by using the objective function (the function with noise)\n",
    "y = asarray([objective(x) for x in X])\n",
    "print('10 generated values for Y with the objective function with extra noise:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reshape X and Y\n",
    "X = X.reshape(len(X), 1)\n",
    "y = y.reshape(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66612502]\n",
      " [0.80710338]\n",
      " [0.05264232]\n",
      " [0.69110573]\n",
      " [0.6545955 ]\n",
      " [0.37860237]\n",
      " [0.66280917]\n",
      " [0.09519841]\n",
      " [0.9073387 ]\n",
      " [0.8109561 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63991744]\n",
      " [ 0.11909587]\n",
      " [-0.02147565]\n",
      " [ 0.2107197 ]\n",
      " [ 0.40697611]\n",
      " [ 0.18079028]\n",
      " [ 0.55677467]\n",
      " [-0.02579045]\n",
      " [-0.01050622]\n",
      " [ 0.08870031]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the model\n",
    "model = GaussianProcessRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# plot the surrogate function that is based on the 10 \n",
    "plot(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of improvement acquisition function\n",
    "def acquisition(X, Xsamples, model):\n",
    "    \n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat, std = surrogate(model, X)\n",
    "    \n",
    "    print('Surrogate called for our data points...')\n",
    "    print(f'X -> {X}')\n",
    "    print(f'yhat -> {yhat}')\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    best = max(yhat)\n",
    "    print(f'Max prediction value is {best}')\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    print('Now we are calling the surrogate function with our random values')\n",
    "    \n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = surrogate(model, Xsamples)\n",
    "    \n",
    "    mu = mu[:, 0]\n",
    "    print('Predictions for our random values')\n",
    "    print(mu)\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    # calculate the probability of improvement for each prediction\n",
    "    probs = norm.cdf((mu - best) / (std+1E-9))\n",
    "    \n",
    "    print('Calculating the probability of improvement')\n",
    "    print(probs)\n",
    "    \n",
    "    return probs\n",
    " \n",
    "# optimize the acquisition function\n",
    "def opt_acquisition(X, y, model):\n",
    "    print('Starting the optimisation for the acquisition for X')\n",
    "    print(X)\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    # random search, generate random samples (we can do this more optimal, but for now let's do random sampling)\n",
    "    Xsamples = random(50)\n",
    "    \n",
    "    print('Generated 50 random samples ..')\n",
    "    print(Xsamples)\n",
    "    \n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(X, Xsamples, model)\n",
    "    \n",
    "    print('The aquisition score for each sample point')\n",
    "    print(scores)\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    # locate the index of the largest scores (argmax returns index of largest value)\n",
    "    ix = argmax(scores)\n",
    "    \n",
    "    print('The index with the largest score')\n",
    "    print(ix)\n",
    "    \n",
    "    \n",
    "    return Xsamples[ix, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.0372491  0.80175169 0.71628145 0.71476753 0.3122789  0.45810861\n",
      " 0.25227703 0.20177523 0.36348108 0.41669132 0.01128    0.38764711\n",
      " 0.54132587 0.30595339 0.44143457 0.10304983 0.50283564 0.57568246\n",
      " 0.35931336 0.4932208  0.1106912  0.52515625 0.53844756 0.96284767\n",
      " 0.06790776 0.46579541 0.85199896 0.5896094  0.5147651  0.16418146\n",
      " 0.66799404 0.69990784 0.13011446 0.23857891 0.46520924 0.36681439\n",
      " 0.34030781 0.86794523 0.6723247  0.69367987 0.16145217 0.15793635\n",
      " 0.89959256 0.61087406 0.21810289 0.27639502 0.9083706  0.91539299\n",
      " 0.98670252 0.62017272]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]]\n",
      "yhat -> [[ 0.17395484]\n",
      " [ 0.07135665]\n",
      " [ 0.01614225]\n",
      " [ 0.06808078]\n",
      " [-0.03411508]\n",
      " [-0.04627752]\n",
      " [ 0.05252254]\n",
      " [ 0.00621235]\n",
      " [ 0.1309405 ]\n",
      " [-0.18593174]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.17395484]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[-0.00860465 -0.1793133  -0.02891266 -0.02596301  0.05310035  0.147443\n",
      "  0.04490578  0.05666041  0.07975066  0.11828691 -0.09025383  0.09660113\n",
      "  0.1736325   0.05096948  0.13632894  0.07483053  0.1691671   0.16304666\n",
      "  0.07705402  0.16576898  0.07691622  0.17368072  0.17386007  0.18372369\n",
      "  0.04782212  0.15212744 -0.20744824  0.15441704  0.17220163  0.07013249\n",
      "  0.05958021  0.00259727  0.07794249  0.0467      0.15178168  0.08195865\n",
      "  0.0658102  -0.1974026   0.05231774  0.01427019  0.07102883  0.07213688\n",
      " -0.1395219   0.13627982  0.05147755  0.04495406 -0.11290443 -0.0878585\n",
      "  0.40037483  0.12648278]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 1.62292607e-222 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 4.06947624e-001 0.00000000e+000 1.72849443e-133 0.00000000e+000\n",
      " 4.27545185e-006 3.48394471e-025 0.00000000e+000 1.10972398e-034\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      " 0.00000000e+000 7.21709217e-076 0.00000000e+000 1.13104504e-144\n",
      " 1.06809403e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 3.40654336e-169 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 1.00000000e+000 0.00000000e+000]\n",
      "The aquisition score for each sample point\n",
      "[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 1.62292607e-222 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 4.06947624e-001 0.00000000e+000 1.72849443e-133 0.00000000e+000\n",
      " 4.27545185e-006 3.48394471e-025 0.00000000e+000 1.10972398e-034\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      " 0.00000000e+000 7.21709217e-076 0.00000000e+000 1.13104504e-144\n",
      " 1.06809403e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 3.40654336e-169 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 1.00000000e+000 0.00000000e+000]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "23\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.9628476692819834 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.9628476692819834 is 0.3413794956837213\n",
      "-------------------------------------------------------------\n",
      "x=0.963, surrogate prediction for x=0.183724, actual return from objective function=0.341\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.72548006 0.92220021 0.58493212 0.36797566 0.18147771 0.84514224\n",
      " 0.8860168  0.5819225  0.1655949  0.35365335 0.65750132 0.27534553\n",
      " 0.59985343 0.46671013 0.19818168 0.14584881 0.66801629 0.14803041\n",
      " 0.89983624 0.49989548 0.95214909 0.05741567 0.39988101 0.96892535\n",
      " 0.09358773 0.40815889 0.982813   0.44914464 0.97968385 0.10983264\n",
      " 0.37859812 0.18686509 0.91159916 0.84259851 0.48753748 0.64581787\n",
      " 0.74904392 0.43339172 0.61551125 0.75756015 0.01648693 0.78586983\n",
      " 0.30558538 0.58164404 0.60762596 0.02345526 0.70479681 0.45571504\n",
      " 0.3752814  0.55956587]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]]\n",
      "yhat -> [[ 0.17925632]\n",
      " [ 0.07057595]\n",
      " [ 0.01601887]\n",
      " [ 0.06906652]\n",
      " [-0.03507102]\n",
      " [-0.04854178]\n",
      " [ 0.05283082]\n",
      " [ 0.00647545]\n",
      " [ 0.12683356]\n",
      " [-0.1837824 ]\n",
      " [ 0.3406046 ]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.3406046]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[-0.04924726 -0.00345033  0.16753483  0.07646871  0.0649606  -0.22198254\n",
      " -0.16194093  0.16927004  0.07061779  0.06739521  0.0851137   0.04217219\n",
      "  0.15694177  0.15128505  0.05866337  0.07608581  0.06710929  0.07561028\n",
      " -0.11515522  0.17029428  0.22995436  0.03197861  0.09984159  0.41059524\n",
      "  0.06980598  0.10635829  0.59130251  0.13865983  0.54796928  0.07636416\n",
      "  0.08384216  0.06293297 -0.06236559 -0.22255719  0.1641354   0.103477\n",
      " -0.10021156  0.12647307  0.14223272 -0.11807007 -0.0664854  -0.17183489\n",
      "  0.04648519  0.16942406  0.15010101 -0.0445019  -0.00484771  0.14353108\n",
      "  0.08148777  0.17806447]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "23\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.9689253538012185 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.9689253538012185 is 0.5385362868932938\n",
      "-------------------------------------------------------------\n",
      "x=0.969, surrogate prediction for x=0.410595, actual return from objective function=0.539\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[7.85789369e-01 2.69517185e-01 9.74324351e-01 3.41826731e-01\n",
      " 2.11232498e-01 8.31008379e-01 5.67582826e-01 9.31409700e-01\n",
      " 2.98131566e-01 4.41949088e-02 8.41352647e-02 1.65935213e-04\n",
      " 7.30706177e-01 3.96428891e-01 7.21213558e-02 2.32265237e-01\n",
      " 6.71807612e-01 8.24175536e-01 2.81046173e-01 4.10062419e-02\n",
      " 9.52348385e-02 6.07398166e-01 7.57063427e-01 2.38772009e-01\n",
      " 1.68192952e-01 7.84628066e-01 7.21921761e-01 3.47311531e-01\n",
      " 6.09237547e-01 5.30537797e-01 4.92097098e-01 5.15436387e-03\n",
      " 9.80290988e-01 2.76090677e-01 9.65669225e-01 4.47140195e-01\n",
      " 8.64195536e-03 4.33058211e-01 5.15279388e-02 4.51443461e-01\n",
      " 5.29635801e-01 9.46651536e-01 5.06625160e-01 4.94632895e-01\n",
      " 8.36568571e-02 1.68349351e-01 3.58137914e-01 2.06207207e-01\n",
      " 8.13950021e-01 1.30337946e-01]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]]\n",
      "yhat -> [[ 0.18065739]\n",
      " [ 0.06833589]\n",
      " [ 0.01616764]\n",
      " [ 0.06957531]\n",
      " [-0.03337288]\n",
      " [-0.04759061]\n",
      " [ 0.05452991]\n",
      " [ 0.00729239]\n",
      " [ 0.12416017]\n",
      " [-0.19299328]\n",
      " [ 0.40276623]\n",
      " [ 0.48327351]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.48327351]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[-0.17961407  0.0434444   0.56017601  0.0595845   0.05561042 -0.23430109\n",
      "  0.17953932  0.07875299  0.04518914  0.00847745  0.06115544 -0.11486244\n",
      " -0.06037784  0.09450912  0.05000842  0.04932988  0.06588042 -0.23057282\n",
      "  0.04342806  0.00213456  0.0684793   0.15607989 -0.12056196  0.04774356\n",
      "  0.07017434 -0.17747295 -0.04017091  0.06234622  0.1543963   0.18088007\n",
      "  0.16577888 -0.09663653  0.65130287  0.04331946  0.43935966  0.13470125\n",
      " -0.08459711  0.12352622  0.02173841  0.13804018  0.18069768  0.21756238\n",
      "  0.17311287  0.16718173  0.06077826  0.07012701  0.06835675  0.05729818\n",
      " -0.22150517  0.07695544]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "2\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.974324350550173 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.974324350550173 is 0.646821223845483\n",
      "-------------------------------------------------------------\n",
      "x=0.974, surrogate prediction for x=0.560176, actual return from objective function=0.647\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.4888893  0.27624675 0.03213038 0.36748202 0.44120052 0.95401868\n",
      " 0.37191714 0.96745822 0.51011921 0.41388545 0.52589953 0.15419023\n",
      " 0.48624859 0.01141572 0.05211099 0.80263768 0.31919394 0.18871831\n",
      " 0.67290012 0.69171252 0.06371169 0.63270994 0.69879933 0.59574369\n",
      " 0.96631425 0.72331236 0.27176728 0.8900361  0.36884841 0.3067291\n",
      " 0.96679125 0.86886311 0.46799941 0.09912322 0.46571251 0.25558474\n",
      " 0.23390077 0.19618071 0.56148653 0.66639996 0.76078257 0.49149685\n",
      " 0.58703765 0.51566121 0.79057726 0.46942495 0.18425608 0.23330211\n",
      " 0.89730111 0.56641466]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]]\n",
      "yhat -> [[ 0.18052721]\n",
      " [ 0.06577516]\n",
      " [ 0.01640153]\n",
      " [ 0.06986499]\n",
      " [-0.03092718]\n",
      " [-0.04561663]\n",
      " [ 0.05657339]\n",
      " [ 0.00823605]\n",
      " [ 0.12225938]\n",
      " [-0.20454597]\n",
      " [ 0.42821765]\n",
      " [ 0.51493812]\n",
      " [ 0.59792328]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.59792328]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.16244435  0.04573059 -0.0143702   0.07376981  0.12807584  0.31424832\n",
      "  0.07650638  0.49336576  0.17373061  0.1065135   0.17956686  0.07339358\n",
      "  0.16081238 -0.06657183  0.02245355 -0.2141428   0.05174351  0.06453013\n",
      "  0.06828165  0.03038812  0.03852546  0.13242316  0.01505661  0.1684556\n",
      "  0.47682953 -0.04123855  0.04583573 -0.16549826  0.07460189  0.04864907\n",
      "  0.48369455 -0.22282815  0.14846849  0.06789732  0.14681101  0.04724979\n",
      "  0.05135202  0.06221509  0.18237257  0.08030105 -0.13033581  0.16401124\n",
      "  0.17370105  0.17605686 -0.1934514   0.14949083  0.06588531  0.0514946\n",
      " -0.1362834   0.18145037]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.48888930245442774 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.48888930245442774 is -0.02902110212186429\n",
      "-------------------------------------------------------------\n",
      "x=0.489, surrogate prediction for x=0.162445, actual return from objective function=-0.029\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.49954125 0.53501788 0.79849896 0.69094824 0.05849051 0.94772444\n",
      " 0.94049048 0.8253791  0.87806306 0.79805936 0.23861189 0.70864824\n",
      " 0.11861492 0.59738996 0.8304245  0.04566891 0.98939209 0.53065932\n",
      " 0.5448665  0.01920126 0.11421442 0.36571155 0.30043016 0.01098595\n",
      " 0.78223111 0.22774851 0.91270709 0.00760948 0.33357993 0.50503731\n",
      " 0.24435998 0.71743264 0.33442247 0.07700118 0.99517807 0.5567575\n",
      " 0.4092275  0.80393759 0.80502266 0.71628865 0.55493927 0.28452377\n",
      " 0.52019362 0.59612332 0.02678268 0.18845953 0.21762217 0.46712218\n",
      " 0.37254924 0.52583456]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]]\n",
      "yhat -> [[ 0.1132865 ]\n",
      " [ 0.0597055 ]\n",
      " [ 0.01641989]\n",
      " [ 0.07339358]\n",
      " [-0.0317347 ]\n",
      " [-0.044451  ]\n",
      " [ 0.06150723]\n",
      " [ 0.01026773]\n",
      " [ 0.06141925]\n",
      " [-0.20245099]\n",
      " [ 0.42543077]\n",
      " [ 0.51446462]\n",
      " [ 0.60002995]\n",
      " [ 0.09331393]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.60002995]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.09922957  0.11531544 -0.19434261  0.02099514  0.02850795  0.23502684\n",
      "  0.15843534 -0.22928381 -0.20073795 -0.19360781  0.05289102 -0.01137137\n",
      "  0.07046652  0.11762047 -0.23308849  0.01301312  0.87342477  0.11374831\n",
      "  0.11831427 -0.0265491   0.06897855  0.03484631  0.03461599 -0.04085231\n",
      " -0.16471815  0.05682874 -0.06070995 -0.04699707  0.03154063  0.10213685\n",
      "  0.05081987 -0.02868223  0.03154564  0.04666972  0.99289465  0.1208024\n",
      "  0.04919553 -0.20306921 -0.20472455 -0.02638745  0.12050843  0.03810716\n",
      "  0.10945463  0.11803341 -0.01418638  0.06953382  0.06043792  0.08056521\n",
      "  0.03637815  0.11185694]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "16\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.9893920881532196 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.9893920881532196 is 0.9132599980649958\n",
      "-------------------------------------------------------------\n",
      "x=0.989, surrogate prediction for x=0.873425, actual return from objective function=0.913\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[7.33190997e-01 3.22776131e-01 8.35031310e-01 6.39277345e-01\n",
      " 1.51809638e-04 5.73395408e-01 9.15686793e-02 1.51487856e-01\n",
      " 8.58171756e-01 2.22165156e-01 3.84352057e-01 1.11130809e-01\n",
      " 6.47863502e-01 1.01825131e-01 7.86824920e-01 1.02265657e-01\n",
      " 9.22172088e-01 6.08941571e-01 5.89363285e-01 8.06140788e-01\n",
      " 7.37964749e-01 9.49360679e-01 5.77226805e-01 8.06756372e-01\n",
      " 9.25150581e-01 3.54570332e-01 7.09526286e-01 7.99649196e-01\n",
      " 2.19611714e-01 9.18468881e-01 3.53826899e-01 4.03810004e-01\n",
      " 9.62404653e-01 2.59810972e-01 2.16391087e-01 6.25559558e-01\n",
      " 5.04427282e-02 8.78451780e-01 8.58807823e-01 6.19977692e-02\n",
      " 6.33827369e-01 5.89068912e-01 3.86160023e-01 7.56218882e-01\n",
      " 1.62077429e-01 3.28611676e-01 1.21852356e-01 2.89309546e-01\n",
      " 7.27449237e-01 2.46372160e-01]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]]\n",
      "yhat -> [[ 0.11277533]\n",
      " [ 0.05718184]\n",
      " [ 0.0166924 ]\n",
      " [ 0.07348204]\n",
      " [-0.02908373]\n",
      " [-0.04206657]\n",
      " [ 0.06357312]\n",
      " [ 0.01120424]\n",
      " [ 0.06073713]\n",
      " [-0.21319747]\n",
      " [ 0.43022013]\n",
      " [ 0.52257586]\n",
      " [ 0.61145616]\n",
      " [ 0.09221077]\n",
      " [ 0.89610267]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.89610267]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[-0.0593071   0.03433847 -0.24313307  0.09379148 -0.04915309  0.12268949\n",
      "  0.05507326  0.07421708 -0.24148464  0.06115246  0.04032421  0.06541777\n",
      "  0.08517838  0.06102943 -0.17584658  0.06125855 -0.00675464  0.11489487\n",
      "  0.12125421 -0.21076703 -0.06973767  0.25241876  0.12262988 -0.21174264\n",
      "  0.01588774  0.03455949 -0.01008821 -0.19990563  0.06197643 -0.03312421\n",
      "  0.03448558  0.04690289  0.42379594  0.04852033  0.0629971   0.10510683\n",
      "  0.01906681 -0.21060324 -0.24098182  0.03114009  0.0986445   0.12131023\n",
      "  0.04084563 -0.11028576  0.07411742  0.03393674  0.06931996  0.0399859\n",
      " -0.0469408   0.05300164]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.733190996797671 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.733190996797671 is -0.03976102792206563\n",
      "-------------------------------------------------------------\n",
      "x=0.733, surrogate prediction for x=-0.059307, actual return from objective function=-0.040\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.10266016 0.80591224 0.30292853 0.90117281 0.02298102 0.39607436\n",
      " 0.34599115 0.02464887 0.6500118  0.00638361 0.13449791 0.65245365\n",
      " 0.86106141 0.21091937 0.38351699 0.46063417 0.32314583 0.49827942\n",
      " 0.71625574 0.45954484 0.91824083 0.3837016  0.83702041 0.98727435\n",
      " 0.67993317 0.87994263 0.15068238 0.7699608  0.25944211 0.7993006\n",
      " 0.50768609 0.27539239 0.98582958 0.85999485 0.24229466 0.14512851\n",
      " 0.02647388 0.41896248 0.97076936 0.68095253 0.73895029 0.98838445\n",
      " 0.10872396 0.26670033 0.29011125 0.85927094 0.88839969 0.25963729\n",
      " 0.64172556 0.33485281]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]]\n",
      "yhat -> [[ 0.11351252]\n",
      " [ 0.05665672]\n",
      " [ 0.01672387]\n",
      " [ 0.07361746]\n",
      " [-0.02293289]\n",
      " [-0.03587818]\n",
      " [ 0.06399512]\n",
      " [ 0.01139092]\n",
      " [ 0.05992341]\n",
      " [-0.21152675]\n",
      " [ 0.42960751]\n",
      " [ 0.52212548]\n",
      " [ 0.61120927]\n",
      " [ 0.0920074 ]\n",
      " [ 0.89676583]\n",
      " [-0.05309653]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.89676583]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.06096113 -0.2054466   0.03707409 -0.13164246 -0.01416874  0.04322958\n",
      "  0.03347707 -0.01196778  0.08758593 -0.03712368  0.07217312  0.0849092\n",
      " -0.23647559  0.0650866   0.0393455   0.07504606  0.03411746  0.09749269\n",
      " -0.01743841  0.07439971 -0.0348345   0.03939605 -0.24045086  0.85318184\n",
      "  0.04815865 -0.20546627  0.074121   -0.13468695  0.04900312 -0.19411755\n",
      "  0.10275745  0.04397082  0.8241148  -0.23741448  0.05481577  0.07371283\n",
      " -0.00958371  0.05254126  0.55180562  0.04656768 -0.06569755  0.87588215\n",
      "  0.0639056   0.04664707  0.03993392 -0.23800683 -0.18156874  0.04893827\n",
      "  0.09594631  0.03340745]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.10266015957436847 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.10266015957436847 is -0.07304178360240127\n",
      "-------------------------------------------------------------\n",
      "x=0.103, surrogate prediction for x=0.060961, actual return from objective function=-0.073\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.13676418 0.88977989 0.87900974 0.94952191 0.92100407 0.4568658\n",
      " 0.27688584 0.43714949 0.53024985 0.479509   0.66142963 0.1271227\n",
      " 0.84844212 0.31102156 0.22189808 0.83820272 0.16061922 0.05246095\n",
      " 0.70763523 0.88843191 0.35787833 0.43403775 0.4364061  0.21710435\n",
      " 0.94994053 0.70780201 0.94505418 0.8113314  0.24745726 0.55585208\n",
      " 0.60769343 0.16822253 0.02260695 0.9621086  0.36162842 0.76173769\n",
      " 0.13895974 0.71416938 0.02050314 0.81867887 0.64778274 0.65905619\n",
      " 0.27145637 0.01278823 0.72813247 0.03979668 0.51725949 0.82022174\n",
      " 0.29530974 0.36048822]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]]\n",
      "yhat -> [[ 0.10162842]\n",
      " [ 0.00412071]\n",
      " [ 0.00886869]\n",
      " [ 0.04601002]\n",
      " [-0.02335417]\n",
      " [-0.03519249]\n",
      " [ 0.06590843]\n",
      " [ 0.01271367]\n",
      " [ 0.07595444]\n",
      " [-0.20923102]\n",
      " [ 0.42592835]\n",
      " [ 0.51965892]\n",
      " [ 0.61016715]\n",
      " [ 0.0900594 ]\n",
      " [ 0.90170848]\n",
      " [-0.05104101]\n",
      " [ 0.00715816]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.90170848]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.02597404 -0.17667997 -0.20566189  0.24849236 -0.01956618  0.08103645\n",
      "  0.07552874  0.0765866   0.10182416  0.08726966  0.06323826  0.0201273\n",
      " -0.23653328  0.07433128  0.06813407 -0.23372126  0.04052687  0.00615883\n",
      " -0.00286043 -0.18092334  0.07118464  0.07598937  0.07644117  0.06672037\n",
      "  0.25351298 -0.00315225  0.19702137 -0.20520139  0.07345331  0.10656965\n",
      "  0.10157359  0.04492235  0.0409348   0.41509807  0.07103443 -0.11039793\n",
      "  0.02732623 -0.01452124  0.04492474 -0.21547401  0.07681799  0.06579769\n",
      "  0.07541788  0.06165123 -0.04099894  0.01639855  0.09843183 -0.21742499\n",
      "  0.07520819  0.07107687]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.13676418343101504 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.13676418343101504 is 0.006117743681022327\n",
      "-------------------------------------------------------------\n",
      "x=0.137, surrogate prediction for x=0.025974, actual return from objective function=0.006\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[5.50288919e-01 2.89239446e-01 3.53273721e-01 9.78925754e-01\n",
      " 9.33248394e-01 3.82178138e-01 8.15667674e-01 8.29269923e-02\n",
      " 6.03252417e-01 8.69835707e-01 4.93249631e-01 6.25501378e-01\n",
      " 7.41045827e-03 2.87483556e-01 5.95116751e-01 6.11568634e-01\n",
      " 5.71104715e-01 4.17466075e-01 9.91314358e-01 2.37764436e-01\n",
      " 4.28553255e-01 1.81084357e-01 9.90719254e-01 3.59455340e-01\n",
      " 5.37800225e-01 3.27329786e-01 1.85708109e-01 8.10775023e-01\n",
      " 9.25345710e-04 2.77420275e-01 8.58596228e-01 5.34912914e-01\n",
      " 1.76226653e-01 2.22573554e-01 8.82107059e-01 8.30997948e-01\n",
      " 3.33849429e-01 3.56578799e-01 1.56941620e-01 2.31354497e-02\n",
      " 5.93068394e-01 4.60988251e-01 2.74222171e-01 4.99582385e-01\n",
      " 3.49834294e-01 2.45003289e-01 5.91932540e-01 4.65876584e-02\n",
      " 2.48331006e-01 1.74067984e-01]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]]\n",
      "yhat -> [[ 1.01083755e-01]\n",
      " [-5.91039658e-04]\n",
      " [ 9.22918320e-03]\n",
      " [ 4.15616035e-02]\n",
      " [-2.34904289e-02]\n",
      " [-3.52550745e-02]\n",
      " [ 6.34560585e-02]\n",
      " [ 1.39577389e-02]\n",
      " [ 7.69929886e-02]\n",
      " [-2.08921313e-01]\n",
      " [ 4.25731301e-01]\n",
      " [ 5.19515514e-01]\n",
      " [ 6.10092163e-01]\n",
      " [ 9.01482105e-02]\n",
      " [ 9.01947141e-01]\n",
      " [-5.10096550e-02]\n",
      " [ 2.11489201e-03]\n",
      " [ 2.06152201e-02]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.90194714]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.1050365   0.07599962  0.07303858  0.6929425   0.07844484  0.07240951\n",
      " -0.21084738 -0.0029937   0.10198951 -0.2217921   0.09138608  0.09221566\n",
      "  0.08639514  0.0759908   0.10412002  0.09904122  0.10659778  0.07462215\n",
      "  0.94358754  0.0704962   0.07613266  0.04784918  0.93058467  0.07277\n",
      "  0.10289156  0.07454622  0.05035615 -0.20376086  0.10785031  0.07574642\n",
      " -0.23304796  0.10229635  0.04511189  0.06625283 -0.1981827  -0.22814882\n",
      "  0.07414281  0.07288778  0.03343225  0.04602146  0.10454512  0.08267856\n",
      "  0.07559109  0.09318674  0.07321072  0.07205164  0.10476303  0.0108577\n",
      "  0.07266998  0.04386473]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "18\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.9913143581692657 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.9913143581692657 is 1.049872241498133\n",
      "-------------------------------------------------------------\n",
      "x=0.991, surrogate prediction for x=0.943588, actual return from objective function=1.050\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.57631558 0.39858248 0.24062359 0.16150856 0.81571881 0.19091885\n",
      " 0.12992493 0.23434779 0.98619389 0.56673403 0.28482471 0.53441424\n",
      " 0.51914554 0.08494964 0.81962399 0.35885217 0.94595721 0.42881289\n",
      " 0.16773502 0.5416277  0.55498444 0.53013359 0.49477472 0.32640318\n",
      " 0.2058656  0.60330025 0.38975367 0.53413877 0.15352391 0.25983336\n",
      " 0.09289719 0.20932727 0.01581493 0.6781408  0.08470093 0.52713379\n",
      " 0.65244973 0.31973447 0.92097804 0.57983185 0.27903215 0.52163937\n",
      " 0.09226625 0.13675994 0.44165511 0.61571287 0.41975151 0.57899617\n",
      " 0.11614032 0.86340971]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]]\n",
      "yhat -> [[ 1.00227952e-01]\n",
      " [-3.20410728e-03]\n",
      " [ 1.00152493e-02]\n",
      " [ 4.29475307e-02]\n",
      " [-1.95618868e-02]\n",
      " [-3.18504572e-02]\n",
      " [ 6.74829483e-02]\n",
      " [ 1.56115294e-02]\n",
      " [ 7.53431320e-02]\n",
      " [-2.29658723e-01]\n",
      " [ 4.32780385e-01]\n",
      " [ 5.32498479e-01]\n",
      " [ 6.28993630e-01]\n",
      " [ 8.80866051e-02]\n",
      " [ 9.40932393e-01]\n",
      " [-4.83878851e-02]\n",
      " [-4.08291817e-04]\n",
      " [ 1.96642876e-02]\n",
      " [ 9.85545158e-01]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.98554516]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.10819924  0.07266474  0.0760138   0.03706002 -0.22202623  0.05582714\n",
      "  0.01500893  0.07438231  0.869205    0.10763907  0.08080077  0.10155988\n",
      "  0.09727871 -0.00528502 -0.22796583  0.07454991  0.20106232  0.0746727\n",
      "  0.04132104  0.10334766  0.10607052  0.10041726  0.08985758  0.07801151\n",
      "  0.06362152  0.10531664  0.07269812  0.10148799  0.03146791  0.07943749\n",
      " -0.00379229  0.06522262  0.07238376  0.04794157 -0.00531089  0.09958649\n",
      "  0.07713819  0.07870305 -0.03669572  0.10821903  0.08075321  0.09801221\n",
      " -0.00395548  0.01966047  0.07656991  0.10130298  0.07371032  0.10822356\n",
      "  0.00638175 -0.24878538]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.5763155810558637 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.5763155810558637 is 0.1333412440977337\n",
      "-------------------------------------------------------------\n",
      "x=0.576, surrogate prediction for x=0.108199, actual return from objective function=0.133\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.99134084 0.9743923  0.95630599 0.10435318 0.25024511 0.86761979\n",
      " 0.67030611 0.10720653 0.84136945 0.15216843 0.31391178 0.99197111\n",
      " 0.50877071 0.06809678 0.82043461 0.76398583 0.94507419 0.36746369\n",
      " 0.04689378 0.89919263 0.75146829 0.33228368 0.9278801  0.41482835\n",
      " 0.42780833 0.22255802 0.15660554 0.34886856 0.53068017 0.16957104\n",
      " 0.62666173 0.85320075 0.25549866 0.86972859 0.54491622 0.54789131\n",
      " 0.04185842 0.69805008 0.27725878 0.28365944 0.67237222 0.42206207\n",
      " 0.4212948  0.70511307 0.93673579 0.39474541 0.27669029 0.89351534\n",
      " 0.48324516 0.04249516]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]]\n",
      "yhat -> [[ 0.10729635]\n",
      " [-0.00125051]\n",
      " [ 0.00940812]\n",
      " [ 0.04191971]\n",
      " [-0.01667273]\n",
      " [-0.02935803]\n",
      " [ 0.0639025 ]\n",
      " [ 0.01440394]\n",
      " [ 0.07669556]\n",
      " [-0.23244262]\n",
      " [ 0.43266296]\n",
      " [ 0.53253853]\n",
      " [ 0.6291461 ]\n",
      " [ 0.09314454]\n",
      " [ 0.9412117 ]\n",
      " [-0.04640818]\n",
      " [ 0.00151563]\n",
      " [ 0.02051222]\n",
      " [ 0.98581648]\n",
      " [ 0.11633503]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.98581648]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.98643875  0.63041091  0.3351078   0.00222921  0.07301784 -0.24755728\n",
      "  0.0637399   0.00350797 -0.25349867  0.03054547  0.07406497  1.00131631\n",
      "  0.10029554 -0.00239074 -0.2315408  -0.1161387   0.18994212  0.070755\n",
      "  0.01081717 -0.16457427 -0.08724761  0.0726999   0.01656067  0.07331562\n",
      "  0.07547081  0.06663322  0.03341901  0.07154906  0.10768127  0.04156256\n",
      "  0.10368526 -0.25636446  0.07372987 -0.24495327  0.11168122  0.11239851\n",
      "  0.01646459  0.02228594  0.0752387   0.07530391  0.06108332  0.07443547\n",
      "  0.07430696  0.00980818  0.09895456  0.07126653  0.07522535 -0.18723273\n",
      "  0.09114635  0.01568961]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.991340836496788 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.991340836496788 is 0.9469167805988433\n",
      "-------------------------------------------------------------\n",
      "x=0.991, surrogate prediction for x=0.986439, actual return from objective function=0.947\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.56012897 0.42784222 0.84998947 0.94009932 0.35977466 0.33226936\n",
      " 0.69348231 0.84542753 0.80651733 0.43370376 0.92356624 0.57433306\n",
      " 0.61961404 0.21345973 0.33775283 0.09607495 0.86594748 0.24717938\n",
      " 0.84010641 0.41805532 0.66048455 0.01195103 0.39801524 0.86856339\n",
      " 0.96262274 0.95712065 0.26559039 0.16846106 0.14518795 0.16502299\n",
      " 0.39051672 0.29330231 0.4842491  0.39334671 0.51029309 0.95889319\n",
      " 0.40568537 0.44287372 0.66133331 0.19788158 0.00352259 0.28912917\n",
      " 0.14937052 0.22959322 0.22426455 0.13805103 0.92333612 0.77160345\n",
      " 0.25991348 0.88900974]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]]\n",
      "yhat -> [[ 1.07654393e-01]\n",
      " [-5.15460968e-04]\n",
      " [ 9.18787718e-03]\n",
      " [ 4.15313840e-02]\n",
      " [-1.76703930e-02]\n",
      " [-3.02226543e-02]\n",
      " [ 6.27622604e-02]\n",
      " [ 1.39404535e-02]\n",
      " [ 7.71607757e-02]\n",
      " [-2.26951003e-01]\n",
      " [ 4.30789739e-01]\n",
      " [ 5.29084295e-01]\n",
      " [ 6.24113351e-01]\n",
      " [ 9.37877893e-02]\n",
      " [ 9.30818439e-01]\n",
      " [-4.70727682e-02]\n",
      " [ 2.22516060e-03]\n",
      " [ 2.07824707e-02]\n",
      " [ 9.74629402e-01]\n",
      " [ 1.16029203e-01]\n",
      " [ 9.75240618e-01]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.97524062]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 1.14753008e-01  7.58865476e-02 -2.51764357e-01  1.36469543e-01\n",
      "  7.04765916e-02  7.17790127e-02  2.86033750e-02 -2.50950933e-01\n",
      " -2.05860496e-01  7.71287680e-02 -1.43762827e-02  1.15990341e-01\n",
      "  1.06397808e-01  6.23774529e-02  7.14479685e-02 -1.72376633e-04\n",
      " -2.44143009e-01  7.11134076e-02 -2.48551369e-01  7.40953684e-02\n",
      "  7.40049481e-02  7.36654401e-02  7.15475678e-02 -2.41137266e-01\n",
      "  4.27322030e-01  3.46154988e-01  7.32479692e-02  4.05303836e-02\n",
      "  2.61006355e-02  3.84756327e-02  7.09720850e-02  7.38057494e-02\n",
      "  9.21522379e-02  7.11662173e-02  1.01371527e-01  3.71546686e-01\n",
      "  7.23457336e-02  7.93156624e-02  7.30565190e-02  5.60483932e-02\n",
      "  9.97490883e-02  7.38801956e-02  2.87520885e-02  6.73930645e-02\n",
      "  6.59097433e-02  2.15896368e-02 -1.61489248e-02 -1.33028626e-01\n",
      "  7.27644563e-02 -1.96986794e-01]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.5601289728134689 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.5601289728134689 is 0.11015422246855364\n",
      "-------------------------------------------------------------\n",
      "x=0.560, surrogate prediction for x=0.114753, actual return from objective function=0.110\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.15548704 0.60112286 0.97581643 0.91446508 0.9116772  0.01285302\n",
      " 0.11553691 0.32024493 0.67353376 0.57540622 0.95818927 0.61721707\n",
      " 0.78246375 0.24185061 0.17098602 0.94036781 0.29942331 0.99930724\n",
      " 0.99147609 0.24205634 0.55230287 0.91557079 0.11536506 0.47343176\n",
      " 0.35587628 0.01216295 0.23953054 0.58645339 0.94025001 0.8806446\n",
      " 0.38137806 0.39883213 0.61038323 0.48283195 0.48436281 0.33810503\n",
      " 0.67464375 0.14521007 0.47563206 0.51489048 0.16230956 0.21983433\n",
      " 0.14419819 0.04430618 0.33730233 0.39861901 0.84397613 0.61423976\n",
      " 0.36016358 0.03931108]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]]\n",
      "yhat -> [[ 1.06636584e-01]\n",
      " [-7.69317150e-04]\n",
      " [ 9.26268101e-03]\n",
      " [ 4.16758060e-02]\n",
      " [-1.79709196e-02]\n",
      " [-3.04709673e-02]\n",
      " [ 6.32213354e-02]\n",
      " [ 1.40948296e-02]\n",
      " [ 7.68326521e-02]\n",
      " [-2.26615727e-01]\n",
      " [ 4.30774570e-01]\n",
      " [ 5.29057860e-01]\n",
      " [ 6.24082118e-01]\n",
      " [ 9.29976702e-02]\n",
      " [ 9.30806845e-01]\n",
      " [-4.72539067e-02]\n",
      " [ 1.97809935e-03]\n",
      " [ 2.06825733e-02]\n",
      " [ 9.74624962e-01]\n",
      " [ 1.14923239e-01]\n",
      " [ 9.75236624e-01]\n",
      " [ 1.13654971e-01]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [0.97523662]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.03263736  0.11212689  0.65167433 -0.07805547 -0.09515166  0.07211626\n",
      "  0.00809574  0.07309413  0.05745602  0.11491013  0.36137509  0.10651791\n",
      " -0.15702844  0.07076204  0.04216498  0.13938981  0.07425058  1.16900519\n",
      "  0.97836214  0.07080197  0.1123507  -0.07096532  0.00800449  0.08776402\n",
      "  0.07094109  0.07404989  0.07029337  0.11455274  0.1381357  -0.21946257\n",
      "  0.07068187  0.07161927  0.10930341  0.09091043  0.09143454  0.07189566\n",
      "  0.0559873   0.02607143  0.08848804  0.10200202  0.0369122   0.06503654\n",
      "  0.02542269  0.01329976  0.07194722  0.07160145 -0.25006837  0.1078074\n",
      "  0.07078743  0.01927304]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "17\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.9993072425248104 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.9993072425248104 is 1.0310232507094106\n",
      "-------------------------------------------------------------\n",
      "x=0.999, surrogate prediction for x=1.169005, actual return from objective function=1.031\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.752013   0.91604916 0.78120797 0.09457912 0.11727907 0.03945821\n",
      " 0.8711351  0.44745021 0.55083599 0.68560425 0.27009051 0.07041103\n",
      " 0.12589859 0.69650539 0.06750854 0.92602507 0.7886409  0.26549215\n",
      " 0.01843857 0.87984125 0.23077141 0.80544542 0.74892788 0.91164209\n",
      " 0.76213301 0.81219884 0.79466239 0.20719705 0.148673   0.87786522\n",
      " 0.15553172 0.99208748 0.39856816 0.86418187 0.05561694 0.23801326\n",
      " 0.41616335 0.12300362 0.16802415 0.66948513 0.0498632  0.61830679\n",
      " 0.36990558 0.09583738 0.52555818 0.18070375 0.49307112 0.81965124\n",
      " 0.14275649 0.22539976]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]]\n",
      "yhat -> [[ 0.10897088]\n",
      " [ 0.00299931]\n",
      " [ 0.00805748]\n",
      " [ 0.0398261 ]\n",
      " [-0.02353692]\n",
      " [-0.03555167]\n",
      " [ 0.05723816]\n",
      " [ 0.01167035]\n",
      " [ 0.07820833]\n",
      " [-0.20039439]\n",
      " [ 0.43416655]\n",
      " [ 0.52634531]\n",
      " [ 0.61519229]\n",
      " [ 0.09610385]\n",
      " [ 0.9004932 ]\n",
      " [-0.05157197]\n",
      " [ 0.00565624]\n",
      " [ 0.02222878]\n",
      " [ 0.94109446]\n",
      " [ 0.11447644]\n",
      " [ 0.94166023]\n",
      " [ 0.11434233]\n",
      " [ 1.12081188]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [1.12081188]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[-9.05251503e-02 -4.09709811e-02 -1.50859177e-01  2.88069248e-03\n",
      "  1.20864511e-02  1.54597163e-02 -2.12096453e-01  8.21340084e-02\n",
      "  1.13330305e-01  3.37083340e-02  6.60341978e-02 -7.10487366e-05\n",
      "  1.64376497e-02  1.65020227e-02  2.93195248e-04  2.99343467e-02\n",
      " -1.65231347e-01  6.57638311e-02  4.56812978e-02 -1.94702446e-01\n",
      "  6.12134933e-02 -1.94496632e-01 -8.40545893e-02 -6.79007173e-02\n",
      " -1.11778855e-01 -2.04528868e-01 -1.76311374e-01  5.50096035e-02\n",
      "  2.86878347e-02 -1.99213743e-01  3.23569179e-02  9.57704365e-01\n",
      "  7.05788136e-02 -2.21645534e-01  3.90207767e-03  6.25798106e-02\n",
      "  7.38891363e-02  1.49439573e-02  3.87754440e-02  5.61263561e-02\n",
      "  7.04371929e-03  1.02321982e-01  6.73155189e-02  3.26704979e-03\n",
      "  1.07936859e-01  4.47681546e-02  9.75636244e-02 -2.14105666e-01\n",
      "  2.54819393e-02  6.00410104e-02]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.7520130037685486 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.7520130037685486 is 0.23838225647795205\n",
      "-------------------------------------------------------------\n",
      "x=0.752, surrogate prediction for x=-0.090525, actual return from objective function=0.238\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.83597941 0.74884008 0.77545557 0.34208115 0.25595188 0.8138018\n",
      " 0.33208292 0.6423052  0.65167062 0.69214531 0.9141353  0.95119344\n",
      " 0.96971083 0.42655132 0.35854251 0.72043939 0.14063895 0.47224689\n",
      " 0.29145565 0.94625929 0.90284503 0.29960479 0.49357424 0.37086219\n",
      " 0.34347404 0.83917039 0.17272161 0.39283169 0.85915155 0.99786831\n",
      " 0.63703985 0.19269057 0.27623073 0.36425663 0.3582482  0.50229978\n",
      " 0.38920262 0.82655114 0.50025096 0.25502478 0.27266208 0.37621679\n",
      " 0.95466326 0.03353937 0.66197634 0.86000272 0.36485732 0.50970073\n",
      " 0.25878036 0.60137821]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]]\n",
      "yhat -> [[ 0.10248446]\n",
      " [-0.00619042]\n",
      " [ 0.01073468]\n",
      " [ 0.04495239]\n",
      " [ 0.04867125]\n",
      " [ 0.03866696]\n",
      " [ 0.07225072]\n",
      " [ 0.01734412]\n",
      " [ 0.06994569]\n",
      " [-0.15413046]\n",
      " [ 0.43121839]\n",
      " [ 0.52240705]\n",
      " [ 0.61096191]\n",
      " [ 0.08465576]\n",
      " [ 0.89890337]\n",
      " [ 0.02496791]\n",
      " [-0.00322831]\n",
      " [ 0.01889873]\n",
      " [ 0.94025874]\n",
      " [ 0.12189341]\n",
      " [ 0.94083643]\n",
      " [ 0.1160357 ]\n",
      " [ 1.12427402]\n",
      " [-0.00993729]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [1.12427402]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[-0.15895128 -0.00381947 -0.05681121  0.07663798  0.08421361 -0.12948942\n",
      "  0.07857227  0.12301159  0.1188736   0.08529258 -0.0317018   0.27974057\n",
      "  0.53484035  0.06915796  0.07355428  0.04631412  0.02191138  0.0786978\n",
      "  0.0847137   0.22420049 -0.08529687  0.08388305  0.08652234  0.07152891\n",
      "  0.07636905 -0.16182184  0.04685199  0.06901729 -0.16889906  1.08967602\n",
      "  0.12478471  0.06046188  0.08541787  0.07257307  0.07360733  0.09017766\n",
      "  0.06931114 -0.14825153  0.08930087  0.08409142  0.08539784  0.07076824\n",
      "  0.32177448  0.03548074  0.11279464 -0.16871333  0.07247448  0.09342945\n",
      "  0.08454812  0.12755215]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.8359794052250893 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.8359794052250893 is 0.12511247401158251\n",
      "-------------------------------------------------------------\n",
      "x=0.836, surrogate prediction for x=-0.158951, actual return from objective function=0.125\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.5661334  0.3340518  0.97399504 0.93856355 0.45419007 0.49946363\n",
      " 0.09986298 0.37162898 0.0958711  0.05423923 0.93393072 0.14568758\n",
      " 0.79630671 0.79865872 0.11491985 0.93514086 0.26365392 0.31963702\n",
      " 0.95132376 0.178766   0.23930722 0.19683178 0.41603204 0.12627062\n",
      " 0.51826513 0.99186224 0.52289606 0.85983777 0.84270291 0.96515848\n",
      " 0.84129061 0.41895078 0.46307427 0.54102634 0.50317699 0.31746714\n",
      " 0.9059146  0.68256275 0.05614662 0.75121356 0.20255107 0.54238183\n",
      " 0.173007   0.47865751 0.79457454 0.16724354 0.91407592 0.74891309\n",
      " 0.72378363 0.27420742]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]]\n",
      "yhat -> [[ 0.09033084]\n",
      " [-0.0124979 ]\n",
      " [ 0.0136528 ]\n",
      " [ 0.0461092 ]\n",
      " [ 0.07162905]\n",
      " [ 0.06510663]\n",
      " [ 0.08420634]\n",
      " [ 0.02192783]\n",
      " [ 0.08282399]\n",
      " [-0.06488967]\n",
      " [ 0.46444631]\n",
      " [ 0.54739189]\n",
      " [ 0.62811089]\n",
      " [ 0.08212376]\n",
      " [ 0.89162803]\n",
      " [ 0.05601406]\n",
      " [-0.0098722 ]\n",
      " [ 0.01414418]\n",
      " [ 0.92959738]\n",
      " [ 0.10425043]\n",
      " [ 0.93012762]\n",
      " [ 0.09940338]\n",
      " [ 1.09888577]\n",
      " [ 0.03217816]\n",
      " [-0.07486868]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [1.09888577]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.10123515  0.1073885   0.62299013  0.20712352  0.08087111  0.08364224\n",
      " -0.01098657  0.0975461  -0.01226449  0.00508666  0.16989255  0.02239561\n",
      " -0.03092813 -0.03418517 -0.00315595  0.1792872   0.10804749  0.11004639\n",
      "  0.32846785  0.05431008  0.09900951  0.07045817  0.08597183  0.00517678\n",
      "  0.08747816  0.94061017  0.08861518 -0.07973719 -0.07855368  0.49502468\n",
      " -0.07790947  0.08537436  0.08063173  0.09361148  0.08428979  0.11036181\n",
      "  0.00971484  0.10025239  0.00271773  0.03325367  0.07515526  0.09401155\n",
      "  0.0488472   0.08114648 -0.02850389  0.04328656  0.04598451  0.03631902\n",
      "  0.06658721  0.11028886]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.5661334024009109 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.5661334024009109 is 0.04475861823038717\n",
      "-------------------------------------------------------------\n",
      "x=0.566, surrogate prediction for x=0.101234, actual return from objective function=0.045\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.20597164 0.9595902  0.03373213 0.1972998  0.03377836 0.83047077\n",
      " 0.63080019 0.648875   0.19084593 0.33381265 0.42894172 0.33033101\n",
      " 0.59373733 0.59129858 0.06679207 0.44352146 0.39485376 0.89708532\n",
      " 0.08451309 0.17559103 0.17021972 0.70673266 0.73002755 0.14871308\n",
      " 0.60473508 0.83878856 0.62611632 0.23840001 0.4381342  0.66494016\n",
      " 0.94966989 0.90876667 0.93135728 0.74562322 0.03677192 0.41264159\n",
      " 0.08292585 0.03495733 0.76905107 0.54466782 0.17174861 0.48879239\n",
      " 0.06842191 0.30855593 0.33879631 0.93708482 0.38325761 0.23181874\n",
      " 0.70015097 0.35242234]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]]\n",
      "yhat -> [[ 0.08052516]\n",
      " [-0.01506019]\n",
      " [ 0.0144105 ]\n",
      " [ 0.04753828]\n",
      " [ 0.06799197]\n",
      " [ 0.06196237]\n",
      " [ 0.08885956]\n",
      " [ 0.02348375]\n",
      " [ 0.08007383]\n",
      " [-0.06242323]\n",
      " [ 0.46398997]\n",
      " [ 0.54688454]\n",
      " [ 0.62761283]\n",
      " [ 0.07470489]\n",
      " [ 0.89149499]\n",
      " [ 0.0534997 ]\n",
      " [-0.01237679]\n",
      " [ 0.01311255]\n",
      " [ 0.92955446]\n",
      " [ 0.09337807]\n",
      " [ 0.93008614]\n",
      " [ 0.0886755 ]\n",
      " [ 1.09933615]\n",
      " [ 0.03108263]\n",
      " [-0.07211614]\n",
      " [ 0.09042454]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [1.09933615]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.08195376  0.42286706  0.04711485  0.07434845  0.04698753 -0.06809521\n",
      "  0.10406685  0.10353994  0.06833315  0.11274457  0.08127165  0.11364317\n",
      "  0.09808731  0.09747195 -0.00873637  0.07802105  0.09187889 -0.01947999\n",
      " -0.01602364  0.05316687  0.04760122  0.07849097  0.05691695  0.02506852\n",
      "  0.10060072 -0.07381439  0.10376716  0.1044631   0.07911181  0.10045481\n",
      "  0.31091714  0.02293229  0.1511426   0.03906274  0.03910255  0.08591509\n",
      " -0.01586223  0.04380512  0.00840044  0.08432484  0.04919338  0.07469964\n",
      " -0.00995278  0.11773372  0.11136103  0.1951189   0.09606338  0.10072207\n",
      "  0.08337903  0.10710144]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.20597163518719808 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.20597163518719808 is -0.07684519624163308\n",
      "-------------------------------------------------------------\n",
      "x=0.206, surrogate prediction for x=0.081954, actual return from objective function=-0.077\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]\n",
      " [0.20597164]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.10830931 0.96944874 0.59340277 0.09021951 0.65903845 0.12862448\n",
      " 0.08614511 0.90650518 0.0300035  0.70669545 0.36307132 0.21379843\n",
      " 0.15207265 0.85008132 0.16394717 0.26214574 0.10910047 0.77919555\n",
      " 0.44477877 0.89391324 0.41879413 0.27428509 0.11473415 0.3500294\n",
      " 0.68268465 0.16780599 0.25639688 0.36886185 0.05597208 0.75154782\n",
      " 0.53567826 0.54660201 0.96982568 0.68136153 0.38156377 0.01967005\n",
      " 0.24166326 0.57563482 0.40103892 0.7287031  0.54931211 0.47992217\n",
      " 0.05820137 0.50927185 0.58529345 0.5772031  0.98714673 0.84258816\n",
      " 0.21824087 0.73169805]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]\n",
      " [0.20597164]]\n",
      "yhat -> [[ 0.08401799]\n",
      " [-0.01306844]\n",
      " [ 0.01792955]\n",
      " [ 0.01292348]\n",
      " [ 0.06690311]\n",
      " [ 0.06027246]\n",
      " [ 0.03531456]\n",
      " [ 0.02503061]\n",
      " [ 0.05700803]\n",
      " [-0.06155968]\n",
      " [ 0.4687345 ]\n",
      " [ 0.55081916]\n",
      " [ 0.63061261]\n",
      " [ 0.06926775]\n",
      " [ 0.89065886]\n",
      " [ 0.05108523]\n",
      " [-0.0131588 ]\n",
      " [-0.00390959]\n",
      " [ 0.92808366]\n",
      " [ 0.10183311]\n",
      " [ 0.92860699]\n",
      " [ 0.0960691 ]\n",
      " [ 1.09483743]\n",
      " [ 0.02725434]\n",
      " [-0.0753727 ]\n",
      " [ 0.09829044]\n",
      " [ 0.03141832]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [1.09483743]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[-0.01260424  0.55826569  0.10688138 -0.01246357  0.1071918  -0.00719881\n",
      " -0.01157546  0.0175941   0.05072808  0.07869315  0.05518603  0.03498912\n",
      "  0.00336456 -0.07986498  0.00958419  0.05093193 -0.01248956 -0.01065969\n",
      "  0.05857801 -0.0259161   0.05552578  0.05317855 -0.01141858  0.05568171\n",
      "  0.09652519  0.01165533  0.04962754  0.05498481  0.00829434  0.02787662\n",
      "  0.0864923   0.09083176  0.56366539  0.09730101  0.05465889  0.07623219\n",
      "  0.04554605  0.10160661  0.05468488  0.05629754  0.09189844  0.06655455\n",
      "  0.00588369  0.07627511  0.10464907  0.10212588  0.84821153 -0.07841039\n",
      "  0.03690672  0.0528419 ]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.10830931277255562 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.10830931277255562 is -0.07794692500459817\n",
      "-------------------------------------------------------------\n",
      "x=0.108, surrogate prediction for x=-0.012604, actual return from objective function=-0.078\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]\n",
      " [0.20597164]\n",
      " [0.10830931]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.43009953 0.78873549 0.43984377 0.17916266 0.85376765 0.89183534\n",
      " 0.92313933 0.84619165 0.14714796 0.99531009 0.6118066  0.73740709\n",
      " 0.44052175 0.58241165 0.33205675 0.44492104 0.15312668 0.01797567\n",
      " 0.79357801 0.10260334 0.97494124 0.91691521 0.66230479 0.03469182\n",
      " 0.64925363 0.84184108 0.43301695 0.39488564 0.66406097 0.23988738\n",
      " 0.9942592  0.9088834  0.6520601  0.63489686 0.57938883 0.54759493\n",
      " 0.90385885 0.14579475 0.56780371 0.62446569 0.88616746 0.34302258\n",
      " 0.54066747 0.58865413 0.43180337 0.44695465 0.47919226 0.19714728\n",
      " 0.5949769  0.04305399]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]\n",
      " [0.20597164]\n",
      " [0.10830931]]\n",
      "yhat -> [[ 0.08314109]\n",
      " [-0.02755189]\n",
      " [ 0.01531625]\n",
      " [ 0.00540853]\n",
      " [ 0.06714511]\n",
      " [ 0.06070375]\n",
      " [ 0.03582907]\n",
      " [ 0.02488327]\n",
      " [ 0.06290483]\n",
      " [-0.06173468]\n",
      " [ 0.46723175]\n",
      " [ 0.54956675]\n",
      " [ 0.62965202]\n",
      " [ 0.07085562]\n",
      " [ 0.89091635]\n",
      " [ 0.05174851]\n",
      " [-0.02795005]\n",
      " [-0.01663303]\n",
      " [ 0.928545  ]\n",
      " [ 0.09942722]\n",
      " [ 0.92907095]\n",
      " [ 0.09401631]\n",
      " [ 1.09627342]\n",
      " [ 0.0283792 ]\n",
      " [-0.07427931]\n",
      " [ 0.09608507]\n",
      " [ 0.03043365]\n",
      " [-0.02740836]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [1.09627342]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.06276417 -0.02247977  0.06323075  0.01188564 -0.07908726 -0.03232956\n",
      "  0.10032678 -0.07855892 -0.01041269  1.01008224  0.10798717  0.04678655\n",
      "  0.06327486  0.10129094  0.06734204  0.06360483 -0.00650215  0.0996182\n",
      " -0.02912211 -0.02795196  0.63923597  0.06495476  0.10453248  0.04619431\n",
      "  0.10777378 -0.07719326  0.06287003  0.06338286  0.10395265  0.04950833\n",
      "  0.98819613  0.02639914  0.10723257  0.10931325  0.10038161  0.08957243\n",
      "  0.00601625 -0.01126885  0.09664965  0.10923982 -0.04582143  0.06694531\n",
      "  0.08709621  0.10306215  0.06282187  0.06378126  0.0686419   0.02455425\n",
      "  0.10468817  0.02603149]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.43009953171176696 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.43009953171176696 is 0.2910107143943937\n",
      "-------------------------------------------------------------\n",
      "x=0.430, surrogate prediction for x=0.062765, actual return from objective function=0.291\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n",
      "Starting the optimisation for the acquisition for X\n",
      "[[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]\n",
      " [0.20597164]\n",
      " [0.10830931]\n",
      " [0.43009953]]\n",
      "-------------------------------------------------------------\n",
      "Generated 50 random samples ..\n",
      "[0.62059113 0.91662438 0.46084437 0.91677816 0.42474451 0.43994113\n",
      " 0.87259853 0.35187679 0.7331534  0.43065226 0.42594043 0.03880074\n",
      " 0.51555229 0.20496408 0.74717623 0.16889983 0.41287788 0.96332433\n",
      " 0.06033441 0.14373669 0.04262174 0.42118256 0.11978879 0.74122363\n",
      " 0.08058022 0.25475719 0.63837256 0.9096989  0.62111538 0.48091013\n",
      " 0.54488354 0.40196037 0.23680622 0.33384199 0.27094853 0.68186292\n",
      " 0.53992346 0.98511258 0.26560765 0.80195042 0.98709069 0.07240917\n",
      " 0.28395813 0.51006233 0.07624064 0.830762   0.62666    0.35192672\n",
      " 0.67975676 0.84183676]\n",
      "-------------------------------------------------------------\n",
      "Surrogate called for our data points...\n",
      "X -> [[0.52944329]\n",
      " [0.09497217]\n",
      " [0.04832441]\n",
      " [0.1701596 ]\n",
      " [0.71894594]\n",
      " [0.72515583]\n",
      " [0.21454034]\n",
      " [0.04358543]\n",
      " [0.43384851]\n",
      " [0.87732235]\n",
      " [0.96284767]\n",
      " [0.96892535]\n",
      " [0.97432435]\n",
      " [0.4888893 ]\n",
      " [0.98939209]\n",
      " [0.733191  ]\n",
      " [0.10266016]\n",
      " [0.13676418]\n",
      " [0.99131436]\n",
      " [0.57631558]\n",
      " [0.99134084]\n",
      " [0.56012897]\n",
      " [0.99930724]\n",
      " [0.752013  ]\n",
      " [0.83597941]\n",
      " [0.5661334 ]\n",
      " [0.20597164]\n",
      " [0.10830931]\n",
      " [0.43009953]]\n",
      "yhat -> [[ 0.1077919 ]\n",
      " [-0.04240608]\n",
      " [ 0.02306676]\n",
      " [ 0.01022673]\n",
      " [ 0.05567431]\n",
      " [ 0.04994678]\n",
      " [ 0.06596756]\n",
      " [ 0.03713202]\n",
      " [ 0.12864828]\n",
      " [-0.05171108]\n",
      " [ 0.46917272]\n",
      " [ 0.55064511]\n",
      " [ 0.63001442]\n",
      " [ 0.11399364]\n",
      " [ 0.88971353]\n",
      " [ 0.04203773]\n",
      " [-0.04332733]\n",
      " [-0.02632332]\n",
      " [ 0.92720509]\n",
      " [ 0.10500002]\n",
      " [ 0.92772889]\n",
      " [ 0.10567331]\n",
      " [ 1.09457541]\n",
      " [ 0.021559  ]\n",
      " [-0.06706548]\n",
      " [ 0.10541058]\n",
      " [ 0.05551839]\n",
      " [-0.04269648]\n",
      " [ 0.12977624]]\n",
      "-------------------------------------------------------------\n",
      "Max prediction value is [1.09457541]\n",
      "-------------------------------------------------------------\n",
      "Now we are calling the surrogate function with our random values\n",
      "Predictions for our random values\n",
      "[ 0.1018033   0.0716188   0.12080359  0.07241201  0.1313858   0.12682009\n",
      " -0.05796981  0.14649773  0.04207587  0.12960982  0.13102794  0.05304456\n",
      "  0.10941458  0.05426812  0.02705336  0.00867057  0.13488674  0.47527337\n",
      " -0.00560236 -0.01977754  0.04019713  0.1324501  -0.03845644  0.03360653\n",
      " -0.03429341  0.10824633  0.09868813  0.0387969   0.10173321  0.11574101\n",
      "  0.10651994  0.13793182  0.09100556  0.14597034  0.12112689  0.08242154\n",
      "  0.10687518  0.80991817  0.11717153 -0.03811264  0.84618163 -0.02539492\n",
      "  0.12952662  0.11019588 -0.02998877 -0.06420469  0.10091424  0.14649534\n",
      "  0.08355856 -0.06931376]\n",
      "-------------------------------------------------------------\n",
      "Calculating the probability of improvement\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "The aquisition score for each sample point\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "-------------------------------------------------------------\n",
      "The index with the largest score\n",
      "0\n",
      "-------------------------------------------------------------\n",
      "We have selected point 0.6205911309373108 for evaluation with our objective function so that we can add it to our belief\n",
      "The actual return for 0.6205911309373108 is 0.359099696571762\n",
      "-------------------------------------------------------------\n",
      "x=0.621, surrogate prediction for x=0.101803, actual return from objective function=0.359\n",
      "Updated our beliefs by training the gaussian process with our previous data and the new sample\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Result: x=0.991, y=1.050\n"
     ]
    }
   ],
   "source": [
    "# plot real observations vs surrogate function\n",
    "def plot(X, y, model):\n",
    "    # scatter plot of inputs and real objective function\n",
    "    pyplot.scatter(X, y)\n",
    "    # line plot of surrogate function across domain\n",
    "    Xsamples = asarray(arange(0, 1, 0.001))\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    ysamples, std = surrogate(model, Xsamples)\n",
    "    pyplot.plot(Xsamples, ysamples)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    " \n",
    "# sample the domain sparsely with noise\n",
    "X = random(10)\n",
    "y = asarray([objective(x) for x in X])\n",
    "\n",
    "\n",
    "# reshape into rows and cols\n",
    "X = X.reshape(len(X), 1)\n",
    "y = y.reshape(len(y), 1)\n",
    "\n",
    "# define the model\n",
    "model = GaussianProcessRegressor()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# plot before hand\n",
    "plot(X, y, model)\n",
    "\n",
    "\n",
    "# perform the optimization process\n",
    "for i in range(20):\n",
    "    # select the next point to sample\n",
    "    x = opt_acquisition(X, y, model)\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    print(f'We have selected point {x} for evaluation with our objective function so that we can add it to our belief')\n",
    "    \n",
    "    # sample the point\n",
    "    actual = objective(x)\n",
    "    print(f'The actual return for {x} is {actual}')\n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    # summarize the finding by calling the current surrogate function with x\n",
    "    est, std = surrogate(model, [[x]])\n",
    "    \n",
    "    print('x=%.3f, surrogate prediction for x=%3f, actual return from objective function=%.3f' % (x, est, actual))\n",
    "    \n",
    "    # add the data to the dataset\n",
    "    X = vstack((X, [[x]]))\n",
    "    y = vstack((y, [[actual]]))\n",
    "    \n",
    "    # update the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    print('Updated our beliefs by training the gaussian process with our previous data and the new sample')\n",
    "    print('-------------------------------------------------------------')\n",
    " \n",
    "# plot all samples and the final surrogate function\n",
    "plot(X, y, model)\n",
    "\n",
    "# best result\n",
    "ix = argmax(y)\n",
    "print('Best Result: x=%.3f, y=%.3f' % (X[ix], y[ix]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
